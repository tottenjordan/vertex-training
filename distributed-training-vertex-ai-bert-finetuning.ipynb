{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b09598e8-8582-4fe8-8e4f-98593c4e6103",
   "metadata": {},
   "source": [
    "# BERT fine-tuning with Vertex AI\n",
    "\n",
    "* Building example from [this notebook example](https://github.com/RajeshThallam/vertex-ai-labs/blob/main/03-distributed-training-text/03-distributed-training-vertex-ai-bert-finetuning.ipynb)\n",
    "\n",
    "This notebook demonstrates how to configure Hyperparameter Tuning and Distributed training in a single training script. \n",
    "\n",
    "After tuning, examples for scaling training code across the following configurations:\n",
    "\n",
    "* 1 replica, 1 GPU\n",
    "* 1 replica, 2 GPUs each\n",
    "* 2 replicas, 1 GPU each\n",
    "* 2 replicas, 1 GPU each + Reduction Server\n",
    "* 2 replicas, 2 GPUs each + Reduction Server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1e2c3a-21f2-4159-bd76-1f731ba9ecfe",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9626a0ad-a786-4052-a36c-da803ddf76cf",
   "metadata": {},
   "source": [
    "### pips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "98d6bd40-e201-410b-bcb4-87b93b2d47f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --user --upgrade google-cloud-aiplatform -q\n",
    "# !pip install --user --upgrade kfp -q\n",
    "# !pip install --user --upgrade google-cloud-pipeline-components -q\n",
    "# !pip install --user --upgrade google-cloud-bigquery-datatransfer -q\n",
    "# !pip install --user tf-models-official==2.11.0 tensorflow-text==2.11.0 -q\n",
    "# pip install tensorflow_io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cc1ceec-8048-42aa-89df-e00b75c77bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import IPython\n",
    "# app = IPython.Application.instance()\n",
    "# app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d449ca34-fec8-402b-8bd8-ce2af363fcaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/raj_sample/vertex-training\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bad248-3a26-413c-9a7e-5ebe2e1ad2ca",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d2c73ae-7a65-4758-9808-be97b1eb7578",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import shutil\n",
    "import sys\n",
    "import pprint\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import google.auth\n",
    "from google.protobuf.json_format import MessageToDict\n",
    "from google.protobuf import json_format\n",
    "from google.protobuf.struct_pb2 import Value\n",
    "\n",
    "\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google.cloud.aiplatform import hyperparameter_tuning as hpt\n",
    "from google.cloud.aiplatform_v1beta1 import types\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import exceptions\n",
    "\n",
    "from google.cloud.aiplatform.utils import JobClientWithOverride\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "from tensorflow_io import bigquery as tfio_bq\n",
    "\n",
    "import logging\n",
    "logging.disable(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714a51eb-9a7a-4f8e-91cc-694e6d85635a",
   "metadata": {},
   "source": [
    "### set vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2090091d-e5e7-42b3-8c73-b9b17a00efeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFIX = 'jtv9'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4a98332-aa40-4934-92a2-5cf7e494eb78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ID     = hybrid-vertex\n",
      "PROJECT_NUM    = 934903580331\n",
      "STAGING_BUCKET = gs://jtv9-hybrid-vertex-bucket\n",
      "VERTEX_SA      = 934903580331-compute@developer.gserviceaccount.com\n"
     ]
    }
   ],
   "source": [
    "# creds, PROJECT_ID = google.auth.default()\n",
    "GCP_PROJECTS             = !gcloud config get-value project\n",
    "PROJECT_ID               = GCP_PROJECTS[0]\n",
    "\n",
    "PROJECT_NUM              = !gcloud projects describe $PROJECT_ID --format=\"value(projectNumber)\"\n",
    "PROJECT_NUM              = PROJECT_NUM[0]\n",
    "\n",
    "VERTEX_SA = f'{PROJECT_NUM}-compute@developer.gserviceaccount.com' # 934903580331\n",
    "\n",
    "REGION = 'us-central1'\n",
    "\n",
    "STAGING_BUCKET = f'gs://{PREFIX}-{PROJECT_ID}-bucket'\n",
    "\n",
    "print(f\"PROJECT_ID     = {PROJECT_ID}\")\n",
    "print(f\"PROJECT_NUM    = {PROJECT_NUM}\")\n",
    "print(f\"STAGING_BUCKET = {STAGING_BUCKET}\")\n",
    "print(f\"VERTEX_SA      = {VERTEX_SA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445cddde-a12c-4b1f-9f40-188ca1ed5a44",
   "metadata": {},
   "source": [
    "### create staging GCS bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07031db4-32d6-4088-a879-a590e884e9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://jtv9-hybrid-vertex-bucket/...\n"
     ]
    }
   ],
   "source": [
    "! gsutil mb -l $REGION $STAGING_BUCKET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a471453e-f45c-4eac-8cbd-e7e1257b23f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TENSORBOARD_NAME = f\"tb-bert-tune-{PREFIX}\"  # @param {type:\"string\"}\n",
    "\n",
    "# if (\n",
    "#     TENSORBOARD_NAME == \"\"\n",
    "#     or TENSORBOARD_NAME is None\n",
    "#     or TENSORBOARD_NAME == \"[your-tensorboard-name]\"\n",
    "# ):\n",
    "#     TENSORBOARD_NAME = PROJECT_ID + \"-tb-\" #+ UUID\n",
    "\n",
    "# tensorboard = vertex_ai.Tensorboard.create(\n",
    "#     display_name=TENSORBOARD_NAME, project=PROJECT_ID, location=REGION\n",
    "# )\n",
    "# TENSORBOARD = tensorboard.gca_resource.name\n",
    "# print(\"TENSORBOARD:\", TENSORBOARD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9e088c-2694-4daa-ac1e-7d3339f991a9",
   "metadata": {},
   "source": [
    "## Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdc2ba8a-80b1-45f6-8a68-bd6639a22ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local_path: /home/jupyter/distributed-training/datasets/aclImdb_v1.tar.gz\n",
      "Downloading data from https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "84125825/84125825 [==============================] - 3s 0us/step\n",
      "dataset_dir: /home/jupyter/distributed-training/datasets/aclImdb\n",
      "train_dir: /home/jupyter/distributed-training/datasets/aclImdb/train\n"
     ]
    }
   ],
   "source": [
    "local_dir = os.path.expanduser('~')\n",
    "local_dir = f'{local_dir}/distributed-training/datasets'\n",
    "\n",
    "if tf.io.gfile.exists(local_dir):\n",
    "    tf.io.gfile.rmtree(local_dir)\n",
    "tf.io.gfile.makedirs(local_dir)\n",
    "\n",
    "url = 'https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "local_path = f'{local_dir}/aclImdb_v1.tar.gz'\n",
    "print(f'local_path: {local_path}')\n",
    "\n",
    "dataset = tf.keras.utils.get_file(\n",
    "    local_path\n",
    "    , url\n",
    "    , untar=True\n",
    "    , cache_dir=local_dir\n",
    "    , cache_subdir='.'\n",
    ")\n",
    "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
    "print(f'dataset_dir: {dataset_dir}')\n",
    "\n",
    "train_dir = os.path.join(dataset_dir, 'train')\n",
    "print(f'train_dir: {train_dir}')\n",
    "\n",
    "# remove unused folders to make it easier to load the data\n",
    "remove_dir = os.path.join(train_dir, 'unsup')\n",
    "shutil.rmtree(remove_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d146508e-7af1-4c44-a4d1-cbd4888b469a",
   "metadata": {},
   "source": [
    "### data splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "856b2f4d-f724-4950-849a-94355c764945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_splits(train_dir, test_dir, val_split, seed):\n",
    "    \n",
    "    train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "        train_dir,\n",
    "        validation_split=val_split,\n",
    "        subset='training',\n",
    "        seed=seed)\n",
    "\n",
    "    class_names = train_ds.class_names\n",
    "    \n",
    "    train_ds = train_ds.unbatch()\n",
    "\n",
    "    val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "        train_dir,\n",
    "        validation_split=val_split,\n",
    "        subset='validation',\n",
    "        seed=seed).unbatch()\n",
    "\n",
    "    test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "        test_dir).unbatch()\n",
    "\n",
    "    return train_ds, val_ds, test_ds, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c85e9bf9-b20e-40b0-ba8d-c681270de80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 20000 files for training.\n",
      "Found 25000 files belonging to 2 classes.\n",
      "Using 5000 files for validation.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "val_split = 0.2\n",
    "test_dir = f'{dataset_dir}/test'\n",
    "\n",
    "train_ds, val_ds, test_ds, class_names = (\n",
    "    create_splits(train_dir, test_dir, val_split, seed)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9bbd940-a361-4801-8377-f25ea6c4cf3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: b'\"Pandemonium\" is a horror movie spoof that comes off more stupid than funny. Believe me when I tell you, I love comedies. Especially comedy spoofs. \"Airplane\", \"The Naked Gun\" trilogy, \"Blazing Saddles\", \"High Anxiety\", and \"Spaceballs\" are some of my favorite comedies that spoof a particular genre. \"Pandemonium\" is not up there with those films. Most of the scenes in this movie had me sitting there in stunned silence because the movie wasn\\'t all that funny. There are a few laughs in the film, but when you watch a comedy, you expect to laugh a lot more than a few times and that\\'s all this film has going for it. Geez, \"Scream\" had more laughs than this film and that was more of a horror film. How bizarre is that?<br /><br />*1/2 (out of four)'\n",
      "Label : 0 (neg)\n",
      "Review: b\"David Mamet is a very interesting and a very un-equal director. His first movie 'House of Games' was the one I liked best, and it set a series of films with characters whose perspective of life changes as they get into complicated situations, and so does the perspective of the viewer.<br /><br />So is 'Homicide' which from the title tries to set the mind of the viewer to the usual crime drama. The principal characters are two cops, one Jewish and one Irish who deal with a racially charged area. The murder of an old Jewish shop owner who proves to be an ancient veteran of the Israeli Independence war triggers the Jewish identity in the mind and heart of the Jewish detective.<br /><br />This is were the flaws of the film are the more obvious. The process of awakening is theatrical and hard to believe, the group of Jewish militants is operatic, and the way the detective eventually walks to the final violent confrontation is pathetic. The end of the film itself is Mamet-like smart, but disappoints from a human emotional perspective.<br /><br />Joe Mantegna and William Macy give strong performances, but the flaws of the story are too evident to be easily compensated.\"\n",
      "Label : 0 (neg)\n"
     ]
    }
   ],
   "source": [
    "for text, label in train_ds.take(2):\n",
    "    print(f'Review: {text.numpy()}')\n",
    "    label = label.numpy()\n",
    "    print(f'Label : {label} ({class_names[label]})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebaa55a-a933-4acc-8c85-b22a58949b22",
   "metadata": {},
   "source": [
    "### create TF Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa44626d-73a5-44a2-b569-804dfbbb0614",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_example(text_fragment, label):\n",
    "    \"\"\"Serializes text fragment and label in tf.Example.\"\"\"\n",
    "    \n",
    "    def _bytes_feature(value):\n",
    "        \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "        if isinstance(value, type(tf.constant(0))):\n",
    "            value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
    "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "    def _int64_feature(value):\n",
    "        \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "        return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "    \n",
    "    feature = {\n",
    "        'text_fragment': _bytes_feature(text_fragment),\n",
    "        'label': _int64_feature(label)\n",
    "    }\n",
    "    \n",
    "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    return example_proto.SerializeToString()\n",
    "    \n",
    "def tf_serialize_example(text_fragment, label):\n",
    "  tf_string = tf.py_function(\n",
    "    serialize_example,\n",
    "    (text_fragment, label), \n",
    "    tf.string)      \n",
    "  return tf.reshape(tf_string, ()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5e9e7da-3973-4ae9-a6b9-7d8eb219765b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfrecords_folder = '{}/tfrecords'.format(os.path.expanduser('~'))\n",
    "if tf.io.gfile.exists(tfrecords_folder):\n",
    "    tf.io.gfile.rmtree(tfrecords_folder)\n",
    "tf.io.gfile.makedirs(tfrecords_folder)\n",
    "\n",
    "filenames = ['train.tfrecords', 'valid.tfrecords', 'test.tfrecords']\n",
    "for file_name, dataset in zip(filenames, [train_ds, val_ds, test_ds]):\n",
    "    writer = tf.data.experimental.TFRecordWriter(os.path.join(tfrecords_folder, file_name))\n",
    "    writer.write(dataset.map(tf_serialize_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7e1c31e-25ea-422c-a8f4-842adef23a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'\\n\\xbd\\x07\\n\\x0e\\n\\x05label\\x12\\x05\\x1a\\x03\\n\\x01\\x01\\n\\xaa\\x07\\n\\rtext_fragment\\x12\\x98\\x07\\n\\x95\\x07\\n\\x92\\x07This scene shows how Wallace\\'s experiment by using his brain manipulation invention goes terribly wrong, creating the \"Were Rabbit\". His desire as a social entrepreneur is to improve society for the better, therefore, created a \"Brain Manipulator\" machine. He risked his own life to help solve Tottington\\'s pests\\' rabbit problem and more importantly to overcome the overcrowding of rabbits being collected and stored in his basement. Though he thought his experiment worked, however, it resulted in placing more pressure on him and Gromit to find a solution before the Annual Vegetable Competition again risking his life. Gromit, who is a silent faithful dog and a loyal helper finds himself continuously thinking of innovative ways to save his master, from his radical crazy inventions going terribly wrong. What is interesting in this movie, is trying to identify: who is more entrepreneurial, Wallace or Gromit?', shape=(), dtype=string)\n",
      "tf.Tensor(b'\\n\\x93\\x06\\n\\x80\\x06\\n\\rtext_fragment\\x12\\xee\\x05\\n\\xeb\\x05\\n\\xe8\\x05I just love this movie and I have my TV programed to record it when it comes on again on Nov. 2nd. It is a really nice love story with a twist. The song that is played at the end of the movie is one you would not think would be a big hit but it is a song that stays in your head and I am now trying to find that song so I can hear it and play it. I really have no style of the shows I see or the songs I like to hear and there for makes me pretty open to seeing things new with an open mind. I would like to say there is some parts in this movie that is not meant for the whole family to watch. This movie does show skin. It is kinda like a lifetime movie for women, about women. I say watch the movie and you may just like it as much as I did.\\n\\x0e\\n\\x05label\\x12\\x05\\x1a\\x03\\n\\x01\\x01', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for record in tf.data.TFRecordDataset([os.path.join(tfrecords_folder, file_name)]).take(2):\n",
    "    print(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd6484d-e5b1-42f1-b0a1-85c434b5ab16",
   "metadata": {},
   "source": [
    "### copy to GCS bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba8783ca-06ce-46b9-a12c-87e8cad11748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file:///home/jupyter/tfrecords/train.tfrecords [Content-Type=application/octet-stream]...\n",
      "/ [1 files][ 26.5 MiB/ 26.5 MiB]                                                \n",
      "Operation completed over 1 objects/26.5 MiB.                                     \n",
      "Copying file:///home/jupyter/tfrecords/valid.tfrecords [Content-Type=application/octet-stream]...\n",
      "/ [1 files][  6.6 MiB/  6.6 MiB]                                                \n",
      "Operation completed over 1 objects/6.6 MiB.                                      \n",
      "Copying file:///home/jupyter/tfrecords/test.tfrecords [Content-Type=application/octet-stream]...\n",
      "/ [1 files][ 32.3 MiB/ 32.3 MiB]                                                \n",
      "Operation completed over 1 objects/32.3 MiB.                                     \n"
     ]
    }
   ],
   "source": [
    "gcs_paths = [f'{STAGING_BUCKET}/bert-finetuning/imdb/tfrecords/train',\n",
    "             f'{STAGING_BUCKET}/bert-finetuning/imdb/tfrecords/valid',\n",
    "             f'{STAGING_BUCKET}/bert-finetuning/imdb/tfrecords/test']\n",
    "\n",
    "for filename, gcs_path in zip(filenames, gcs_paths):\n",
    "    local_file_path = os.path.join(tfrecords_folder, filename)\n",
    "    gcs_file_path = f'{gcs_path}/{filename}'\n",
    "    !gsutil cp {local_file_path} {gcs_file_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3713ba67-e2d4-4cc9-b1b8-38a70c4e1dfc",
   "metadata": {},
   "source": [
    "## Create Training package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ceff24-2500-40e7-8db6-e6e26189b8fc",
   "metadata": {},
   "source": [
    "### base image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "008c8aa2-e0c6-4fcd-8a77-7c9afc9bc06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BASE_IMAGE = 'us-docker.pkg.dev/vertex-ai/training/tf-gpu.2-11:latest'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f7138b-865a-4c1d-862a-6a176018187f",
   "metadata": {},
   "source": [
    "### create train dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d51adb5-df0a-49ca-913b-4b95a70930a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf scripts/trainer\n",
    "! mkdir -p scripts/trainer\n",
    "! touch scripts/trainer/__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1121a6df-5ed6-4e31-94e4-5cff725a6871",
   "metadata": {},
   "source": [
    "### training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea7c3847-9f27-4e33-8919-058c2054181d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing scripts/trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/trainer/task.py\n",
    "# Copyright 2021 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#            http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "\n",
    "import time\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from absl import logging\n",
    "from official.nlp import optimization \n",
    "\n",
    "import random\n",
    "import string\n",
    "\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from hypertune import HyperTune\n",
    "\n",
    "\n",
    "TFHUB_HANDLE_ENCODER = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3'\n",
    "TFHUB_HANDLE_PREPROCESS = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\n",
    "LOCAL_TB_FOLDER = '/tmp/logs'\n",
    "LOCAL_SAVED_MODEL_DIR = '/tmp/saved_model'\n",
    "\n",
    "# ====================================================\n",
    "# training args\n",
    "# ====================================================\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_integer('steps_per_epoch', 625, 'Steps per training epoch')\n",
    "flags.DEFINE_integer('eval_steps', 150, 'Evaluation steps')\n",
    "flags.DEFINE_integer('epochs', 2, 'Nubmer of epochs')\n",
    "flags.DEFINE_integer('per_replica_batch_size', 32, 'Per replica batch size')\n",
    "flags.DEFINE_integer('TRAIN_NGPU', 1, '')\n",
    "flags.DEFINE_integer('replica_count', 1, '')\n",
    "flags.DEFINE_integer('reduction_cnt', 0, '')\n",
    "\n",
    "flags.DEFINE_float('learning_rate', 0.001, '')\n",
    "\n",
    "flags.DEFINE_string('training_data_path', f'/bert-finetuning/imdb/tfrecords/train', 'Training data GCS path')\n",
    "flags.DEFINE_string('validation_data_path', f'/bert-finetuning/imdb/tfrecords/valid', 'Validation data GCS path')\n",
    "flags.DEFINE_string('testing_data_path', f'/bert-finetuning/imdb/tfrecords/test', 'Testing data GCS path')\n",
    "\n",
    "flags.DEFINE_string('job_dir', f'/jobs', 'A base GCS path for jobs')\n",
    "flags.DEFINE_string('job_id', 'default', 'unique_id for experiment runs')\n",
    "flags.DEFINE_string('TRAIN_GPU', 'NA', '')\n",
    "flags.DEFINE_string('experiment_run', 'NA', '')\n",
    "flags.DEFINE_string('experiment_name', 'NA', '')\n",
    "flags.DEFINE_string('tuning', 'False', 'Tune model hyper parameters?')\n",
    "\n",
    "\n",
    "flags.DEFINE_enum('strategy', 'multiworker', ['single', 'mirrored', 'multiworker'], 'Distribution strategy')\n",
    "flags.DEFINE_enum('auto_shard_policy', 'auto', ['auto', 'data', 'file', 'off'], 'Dataset sharing strategy')\n",
    "\n",
    "auto_shard_policy = {\n",
    "    'auto': tf.data.experimental.AutoShardPolicy.AUTO,\n",
    "    'data': tf.data.experimental.AutoShardPolicy.DATA,\n",
    "    'file': tf.data.experimental.AutoShardPolicy.FILE,\n",
    "    'off': tf.data.experimental.AutoShardPolicy.OFF,\n",
    "}\n",
    "\n",
    "# ====================================================\n",
    "# helper functions\n",
    "# ====================================================\n",
    "\n",
    "def create_unbatched_dataset(tfrecords_folder):\n",
    "    \"\"\"Creates an unbatched dataset in the format required by the \n",
    "       sentiment analysis model from the folder with TFrecords files.\"\"\"\n",
    "    \n",
    "    feature_description = {\n",
    "        'text_fragment': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
    "        'label': tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
    "    }\n",
    "\n",
    "    def _parse_function(example_proto):\n",
    "        parsed_example = tf.io.parse_single_example(example_proto, feature_description)\n",
    "        return parsed_example['text_fragment'], parsed_example['label']\n",
    "  \n",
    "    file_paths = [f'{tfrecords_folder}/{file_path}' for file_path in tf.io.gfile.listdir(tfrecords_folder)]\n",
    "    dataset = tf.data.TFRecordDataset(file_paths)\n",
    "    dataset = dataset.map(_parse_function)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def configure_dataset(ds, auto_shard_policy):\n",
    "    \"\"\"\n",
    "    Optimizes the performance of a dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    options = tf.data.Options()\n",
    "    options.experimental_distribute.auto_shard_policy = (\n",
    "        auto_shard_policy\n",
    "    )\n",
    "    \n",
    "    ds = ds.repeat(-1).cache()\n",
    "    ds = ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    ds = ds.with_options(options)\n",
    "    return ds\n",
    "\n",
    "\n",
    "def create_input_pipelines(train_dir, valid_dir, test_dir, batch_size, auto_shard_policy):\n",
    "    \"\"\"Creates input pipelines from Imdb dataset.\"\"\"\n",
    "    \n",
    "    train_ds = create_unbatched_dataset(train_dir)\n",
    "    train_ds = train_ds.batch(batch_size)\n",
    "    train_ds = configure_dataset(train_ds, auto_shard_policy)\n",
    "    \n",
    "    valid_ds = create_unbatched_dataset(valid_dir)\n",
    "    valid_ds = valid_ds.batch(batch_size)\n",
    "    valid_ds = configure_dataset(valid_ds, auto_shard_policy)\n",
    "    \n",
    "    test_ds = create_unbatched_dataset(test_dir)\n",
    "    test_ds = test_ds.batch(batch_size)\n",
    "    test_ds = configure_dataset(test_ds, auto_shard_policy)\n",
    "\n",
    "    return train_ds, valid_ds, test_ds\n",
    "\n",
    "\n",
    "def build_classifier_model(tfhub_handle_preprocess, tfhub_handle_encoder):\n",
    "    \"\"\"Builds a simple binary classification model with BERT trunk.\"\"\"\n",
    "    \n",
    "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
    "    encoder_inputs = preprocessing_layer(text_input)\n",
    "    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
    "    outputs = encoder(encoder_inputs)\n",
    "    net = outputs['pooled_output']\n",
    "    net = tf.keras.layers.Dropout(0.1)(net)\n",
    "    net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n",
    "    \n",
    "    return tf.keras.Model(text_input, net)\n",
    "\n",
    "\n",
    "def copy_tensorboard_logs(local_path: str, gcs_path: str):\n",
    "    \"\"\"Copies Tensorboard logs from a local dir to a GCS location.\n",
    "    \n",
    "    After training, batch copy Tensorboard logs locally to a GCS location. This can result\n",
    "    in faster pipeline runtimes over streaming logs per batch to GCS that can get bottlenecked\n",
    "    when streaming large volumes.\n",
    "    \n",
    "    Args:\n",
    "      local_path: local filesystem directory uri.\n",
    "      gcs_path: cloud filesystem directory uri.\n",
    "    Returns:\n",
    "      None.\n",
    "    \"\"\"\n",
    "    pattern = '{}/*/events.out.tfevents.*'.format(local_path)\n",
    "    local_files = tf.io.gfile.glob(pattern)\n",
    "    gcs_log_files = [local_file.replace(local_path, gcs_path) for local_file in local_files]\n",
    "    for local_file, gcs_file in zip(local_files, gcs_log_files):\n",
    "        tf.io.gfile.copy(local_file, gcs_file)\n",
    "\n",
    "# ====================================================\n",
    "# training main\n",
    "# ====================================================\n",
    "\n",
    "def main(argv):\n",
    "    del argv\n",
    "    \n",
    "    def _is_chief(task_type, task_id):\n",
    "        return ((task_type == 'chief' or task_type == 'worker') and task_id == 0) or task_type is None\n",
    "        \n",
    "    # ====================================================\n",
    "    # set args\n",
    "    # ====================================================\n",
    "    \n",
    "    logging.info('Setting up training.')\n",
    "    logging.info('   epochs: {}'.format(FLAGS.epochs))\n",
    "    logging.info('   steps_per_epoch: {}'.format(FLAGS.steps_per_epoch))\n",
    "    logging.info('   eval_steps: {}'.format(FLAGS.eval_steps))\n",
    "    logging.info('   strategy: {}'.format(FLAGS.strategy))\n",
    "    logging.info('   job_id: {}'.format(FLAGS.job_id))\n",
    "    logging.info('   TRAIN_GPU: {}'.format(FLAGS.TRAIN_GPU))\n",
    "    logging.info('   TRAIN_NGPU: {}'.format(FLAGS.TRAIN_NGPU))\n",
    "    logging.info('   replica_count: {}'.format(FLAGS.replica_count))\n",
    "    logging.info('   reduction_cnt: {}'.format(FLAGS.reduction_cnt))\n",
    "    logging.info('   experiment_name: {}'.format(FLAGS.experiment_name))\n",
    "    logging.info('   experiment_run: {}'.format(FLAGS.experiment_run))\n",
    "    logging.info('   learning_rate: {}'.format(FLAGS.learning_rate))\n",
    "    logging.info('   tuning: {}'.format(FLAGS.tuning))\n",
    "    \n",
    "    tb_dir = os.getenv('AIP_TENSORBOARD_LOG_DIR', LOCAL_TB_FOLDER)\n",
    "    model_dir = os.getenv('AIP_MODEL_DIR', LOCAL_SAVED_MODEL_DIR)\n",
    "    logging.info(f'AIP_TENSORBOARD_LOG_DIR = {tb_dir}')\n",
    "    logging.info(f'AIP_MODEL_DIR = {model_dir}')\n",
    "    \n",
    "    project_number = os.environ[\"CLOUD_ML_PROJECT_ID\"]\n",
    "    \n",
    "    vertex_ai.init(\n",
    "        project=project_number,\n",
    "        location='us-central1',\n",
    "        experiment=FLAGS.experiment_name\n",
    "    )\n",
    "\n",
    "    # ====================================================\n",
    "    # set distribution strategy (tensorflow) \n",
    "    # ====================================================\n",
    "    logging.info('DEVICES'  + str(device_lib.list_local_devices()))\n",
    "    \n",
    "    # Single Machine, single compute device\n",
    "    if FLAGS.strategy == 'single':\n",
    "        if tf.test.is_gpu_available():\n",
    "            strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n",
    "        else:\n",
    "            strategy = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")\n",
    "        logging.info(\"Single device training\")\n",
    "    \n",
    "    # Single Machine, multiple compute device\n",
    "    elif FLAGS.strategy == 'mirrored':\n",
    "        strategy = tf.distribute.MirroredStrategy()\n",
    "        logging.info(\"Mirrored Strategy distributed training\")\n",
    "    \n",
    "    # Multi Machine, multiple compute device\n",
    "    elif FLAGS.strategy == 'multiworker':\n",
    "        strategy = tf.distribute.MultiWorkerMirroredStrategy()\n",
    "        logging.info(\"Multi-worker Strategy distributed training\")\n",
    "        logging.info('TF_CONFIG = {}'.format(os.environ.get('TF_CONFIG', 'Not found')))\n",
    "   \n",
    "    # Single Machine, multiple TPU devices\n",
    "    elif FLAGS.strategy == 'tpu':\n",
    "        cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=\"local\")\n",
    "        tf.config.experimental_connect_to_cluster(cluster_resolver)\n",
    "        tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\n",
    "        strategy = tf.distribute.TPUStrategy(cluster_resolver)\n",
    "        print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n",
    "\n",
    "    logging.info('num_replicas_in_sync = {}'.format(strategy.num_replicas_in_sync))\n",
    "    \n",
    "    if strategy.cluster_resolver:    \n",
    "        task_type, task_id = (strategy.cluster_resolver.task_type,\n",
    "                              strategy.cluster_resolver.task_id)\n",
    "    else:\n",
    "        task_type, task_id = (None, None)\n",
    "        \n",
    "    logging.info('task_type = {}'.format(task_type))\n",
    "    logging.info('task_id = {}'.format(task_id))\n",
    "    \n",
    "    global_batch_size = (\n",
    "        strategy.num_replicas_in_sync *\n",
    "        FLAGS.per_replica_batch_size\n",
    "    )\n",
    "    \n",
    "    # ====================================================\n",
    "    # data input pipeline\n",
    "    # ====================================================\n",
    "    \n",
    "    train_ds, valid_ds, test_ds = create_input_pipelines(\n",
    "        FLAGS.training_data_path,\n",
    "        FLAGS.validation_data_path,\n",
    "        FLAGS.testing_data_path,\n",
    "        global_batch_size,\n",
    "        auto_shard_policy[FLAGS.auto_shard_policy]\n",
    "    )\n",
    "        \n",
    "    num_train_steps = FLAGS.steps_per_epoch * FLAGS.epochs\n",
    "    num_warmup_steps = int(0.1*num_train_steps)\n",
    "    init_lr = FLAGS.learning_rate # FLAGS.learning_rate 3e-5\n",
    "    \n",
    "    # ====================================================\n",
    "    # build & compile model\n",
    "    # ====================================================\n",
    "    \n",
    "    with strategy.scope():\n",
    "        \n",
    "        model = build_classifier_model(TFHUB_HANDLE_PREPROCESS, TFHUB_HANDLE_ENCODER)\n",
    "        \n",
    "        loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "        \n",
    "        metrics = tf.metrics.BinaryAccuracy()\n",
    "        \n",
    "        optimizer = optimization.create_optimizer(\n",
    "            init_lr=init_lr\n",
    "            , num_train_steps=num_train_steps\n",
    "            , num_warmup_steps=num_warmup_steps\n",
    "            , optimizer_type='adamw'\n",
    "        )\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=optimizer\n",
    "            , loss=loss\n",
    "            , metrics=metrics\n",
    "        )\n",
    "        \n",
    "    # ====================================================\n",
    "    # set callbacks\n",
    "    # ====================================================\n",
    "        \n",
    "    # Configure BackupAndRestore callback\n",
    "    if FLAGS.strategy == 'single':\n",
    "        callbacks = []\n",
    "        logging.info(\"No backup and restore\")\n",
    "    else:\n",
    "        backup_dir = '{}/backupandrestore'.format(FLAGS.job_dir)\n",
    "        callbacks = [tf.keras.callbacks.experimental.BackupAndRestore(backup_dir=backup_dir)]\n",
    "        logging.info(f\"saved backup and restore t0: {backup_dir}\")\n",
    "    \n",
    "    # Configure TensorBoard callback on Chief\n",
    "    if _is_chief(task_type, task_id):\n",
    "        callbacks.append(\n",
    "            tf.keras.callbacks.TensorBoard(\n",
    "                log_dir=tb_dir\n",
    "                , update_freq='batch'\n",
    "                , histogram_freq=1\n",
    "            )\n",
    "        )\n",
    "        \n",
    "#     if FLAGS.tuning == \"True\":\n",
    "#         # Instantiate the HyperTune reporting object\n",
    "#         hpt = HyperTune()\n",
    "\n",
    "#         # Reporting callback\n",
    "#         class HPTCallback(tf.keras.callbacks.Callback):\n",
    "\n",
    "#             def on_epoch_end(self, epoch, logs=None):\n",
    "#                 hpt.report_hyperparameter_tuning_metric(\n",
    "#                     hyperparameter_metric_tag='binary_accuracy',\n",
    "#                     metric_value=logs['val_binary_accuracy'],\n",
    "#                     global_step=epoch\n",
    "#                 )\n",
    "\n",
    "#         if not callbacks:\n",
    "#             callbacks = []\n",
    "#         callbacks.append(HPTCallback())\n",
    "    \n",
    "    # ====================================================\n",
    "    # train model\n",
    "    # ====================================================\n",
    "    \n",
    "    logging.info('Starting training ...')\n",
    "    \n",
    "    if _is_chief(task_type, task_id):\n",
    "        start_time = time.time()\n",
    "    \n",
    "    history = model.fit(\n",
    "        x=train_ds\n",
    "        , validation_data=valid_ds\n",
    "        , steps_per_epoch=FLAGS.steps_per_epoch\n",
    "        , validation_steps=FLAGS.eval_steps\n",
    "        , epochs=FLAGS.epochs\n",
    "        , callbacks=callbacks\n",
    "    )\n",
    "    \n",
    "    # ====================================================\n",
    "    # log Vertex Experiments\n",
    "    # ====================================================\n",
    "    \n",
    "    SESSION_id = \"\".join(random.choices(string.ascii_lowercase + string.digits, k=3))\n",
    "    \n",
    "    if _is_chief(task_type, task_id):\n",
    "        end_time = time.time()\n",
    "        # val metrics\n",
    "        val_keys = [v for v in history.history.keys()]\n",
    "        total_train_time = int((end_time - start_time) / 60)\n",
    "\n",
    "        metrics_dict = {\"total_train_time\": total_train_time}\n",
    "        logging.info(f\"total_train_time: {total_train_time}\")\n",
    "        _ = [metrics_dict.update({key: history.history[key][-1]}) for key in val_keys]\n",
    "    \n",
    "        logging.info(f\" task_type logging experiments: {task_type}\")\n",
    "        logging.info(f\" task_id logging experiments: {task_id}\")\n",
    "        logging.info(f\" logging data to experiment run: {FLAGS.experiment_run}-{SESSION_id}\")\n",
    "        \n",
    "        with vertex_ai.start_run(\n",
    "            f'{FLAGS.experiment_run}-{SESSION_id}', \n",
    "        ) as my_run:\n",
    "            \n",
    "            logging.info(f\"logging metrics...\")\n",
    "            my_run.log_metrics(metrics_dict)\n",
    "\n",
    "            logging.info(f\"logging metaparams...\")\n",
    "            my_run.log_params(\n",
    "                {\n",
    "                    \"epochs\": FLAGS.epochs,\n",
    "                    \"strategy\": FLAGS.strategy,\n",
    "                    \"per_replica_batch_size\": FLAGS.per_replica_batch_size,\n",
    "                    \"TRAIN_GPU\": FLAGS.TRAIN_GPU,\n",
    "                    \"TRAIN_NGPU\": FLAGS.TRAIN_NGPU,\n",
    "                    \"replica_count\": FLAGS.replica_count,\n",
    "                    \"reduction_cnt\": FLAGS.reduction_cnt,\n",
    "                    \"global_batch_size\": global_batch_size,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            vertex_ai.end_run()\n",
    "            logging.info(f\"EXPERIMENT RUN: '{FLAGS.experiment_run}-{SESSION_id}' has ended\")\n",
    "            \n",
    "    # ====================================================\n",
    "    # save model\n",
    "    # ====================================================\n",
    "\n",
    "    if FLAGS.strategy==\"tpu\":\n",
    "        logging.info(f\"Training completed. Saving TPU trained model...\")\n",
    "        save_locally = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')\n",
    "        model.save(model_dir, options=save_locally)\n",
    "    # single, mirrored or primary for multiworker\n",
    "    elif _is_chief(task_type, task_id):\n",
    "        logging.info('Training completed. Saving the trained model to: {}'.format(model_dir))\n",
    "        model.save(model_dir)\n",
    "    # non-primary workers for multi-workers\n",
    "    else:\n",
    "        # each worker saves their model instance to a unique temp location\n",
    "        worker_dir = model_dir + '/workertemp_' + str(task_id)\n",
    "        tf.io.gfile.makedirs(worker_dir)\n",
    "        model.save(worker_dir)\n",
    "        logging.info(f\"worker saved to temp worker_dir: {worker_dir} ...\")\n",
    "        \n",
    "        logging.info(f\"recursively deleting everything under path: {worker_dir} ...\")\n",
    "        tf.io.gfile.rmtree(worker_dir)\n",
    "\n",
    "    logging.info('Models saved!')\n",
    "        \n",
    "    # Save trained model\n",
    "    # saved_model_dir = '{}/saved_model'.format(model_dir)\n",
    "    # logging.info('Training completed. Saving the trained model to: {}'.format(saved_model_dir))\n",
    "    # model.save(saved_model_dir)\n",
    "    #tf.saved_model.save(model, saved_model_dir)\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    logging.set_verbosity(logging.INFO)\n",
    "    app.run(main)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fa47fb-260f-4ff9-aa4c-620194771296",
   "metadata": {},
   "source": [
    "### Dockferfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "085d6600-0e5e-48cc-932d-b8cb01444e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_IMAGE = f'gcr.io/{PROJECT_ID}/imdb_bert'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "290b6bc2-797c-400a-b814-167a59106e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "dockerfile = f'''\n",
    "FROM {TRAIN_BASE_IMAGE}\n",
    "\n",
    "RUN pip install tf-models-official==2.11.0\n",
    "RUN pip install tensorflow-text==2.11.0\n",
    "RUN pip install cloudml-hypertune\n",
    "\n",
    "WORKDIR /\n",
    "\n",
    "# Copies the trainer code to the docker image.\n",
    "COPY trainer /trainer\n",
    "\n",
    "RUN apt update && apt -y install nvtop\n",
    "\n",
    "# Sets up the entry point to invoke the trainer.\n",
    "ENTRYPOINT [\"python\", \"-m\", \"trainer.task\"]\n",
    "'''\n",
    "\n",
    "with open('scripts/Dockerfile', 'w') as f:\n",
    "    f.write(dockerfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5893556b-691e-429d-b852-c32d29a0e1f2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  22.02kB\n",
      "Step 1/8 : FROM us-docker.pkg.dev/vertex-ai/training/tf-gpu.2-11:latest\n",
      " ---> 1fc7736bd93e\n",
      "Step 2/8 : RUN pip install tf-models-official==2.11.0\n",
      " ---> Using cache\n",
      " ---> f4b45eeb250d\n",
      "Step 3/8 : RUN pip install tensorflow-text==2.11.0\n",
      " ---> Using cache\n",
      " ---> 2a27018a1165\n",
      "Step 4/8 : RUN pip install cloudml-hypertune\n",
      " ---> Using cache\n",
      " ---> 71a3be94b595\n",
      "Step 5/8 : WORKDIR /\n",
      " ---> Using cache\n",
      " ---> fb7eb9e35db6\n",
      "Step 6/8 : COPY trainer /trainer\n",
      " ---> 907ff3eec1b8\n",
      "Step 7/8 : RUN apt update && apt -y install nvtop\n",
      " ---> Running in 6a75d8981f99\n",
      "\u001b[91m\n",
      "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
      "\n",
      "\u001b[0mGet:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
      "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1581 B]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy InRelease [270 kB]\n",
      "Get:4 http://packages.cloud.google.com/apt cloud-sdk InRelease [6361 B]\n",
      "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [361 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
      "Get:7 http://packages.cloud.google.com/apt cloud-sdk/main amd64 Packages [464 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [108 kB]\n",
      "Get:9 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [938 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu jammy/universe amd64 Packages [17.5 MB]\n",
      "Get:11 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [36.3 kB]\n",
      "Get:12 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [631 kB]\n",
      "Get:13 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [541 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 Packages [266 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu jammy/main amd64 Packages [1792 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu jammy/restricted amd64 Packages [164 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [42.2 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [922 kB]\n",
      "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [545 kB]\n",
      "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1191 kB]\n",
      "Get:21 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [49.4 kB]\n",
      "Get:22 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [25.5 kB]\n",
      "Fetched 26.1 MB in 2s (12.9 MB/s)\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "77 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "\u001b[91m\n",
      "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
      "\n",
      "\u001b[0mReading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "The following NEW packages will be installed:\n",
      "  nvtop\n",
      "0 upgraded, 1 newly installed, 0 to remove and 77 not upgraded.\n",
      "Need to get 43.9 kB of archives.\n",
      "After this operation, 106 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 nvtop amd64 1.2.2-1 [43.9 kB]\n",
      "\u001b[91mdebconf: delaying package configuration, since apt-utils is not installed\n",
      "\u001b[0mFetched 43.9 kB in 0s (315 kB/s)\n",
      "Selecting previously unselected package nvtop.\n",
      "(Reading database ... 74622 files and directories currently installed.)\n",
      "Preparing to unpack .../nvtop_1.2.2-1_amd64.deb ...\n",
      "Unpacking nvtop (1.2.2-1) ...\n",
      "Setting up nvtop (1.2.2-1) ...\n",
      "Removing intermediate container 6a75d8981f99\n",
      " ---> f4b8fc151f2f\n",
      "Step 8/8 : ENTRYPOINT [\"python\", \"-m\", \"trainer.task\"]\n",
      " ---> Running in 68926e068ff8\n",
      "Removing intermediate container 68926e068ff8\n",
      " ---> 8d05525f62f5\n",
      "Successfully built 8d05525f62f5\n",
      "Successfully tagged gcr.io/hybrid-vertex/imdb_bert:latest\n"
     ]
    }
   ],
   "source": [
    "! docker build -t {TRAIN_IMAGE} scripts/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09fd8567-d1e4-4c1d-bc2f-acdbf1664b89",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default tag: latest\n",
      "The push refers to repository [gcr.io/hybrid-vertex/imdb_bert]\n",
      "\n",
      "\u001b[1B441a2243: Preparing \n",
      "\u001b[1B79440985: Preparing \n",
      "\u001b[1B13c176db: Preparing \n",
      "\u001b[1B4e30591d: Preparing \n",
      "\u001b[1Ba416ba09: Preparing \n",
      "\u001b[1B95c7b436: Preparing \n",
      "\u001b[1Bf663075e: Preparing \n",
      "\u001b[1B7fbea69f: Preparing \n",
      "\u001b[1Bdcb6c992: Preparing \n",
      "\u001b[1B57d3600a: Preparing \n",
      "\u001b[1B98aef500: Preparing \n",
      "\u001b[1B937e6451: Preparing \n",
      "\u001b[1Bf16690a3: Preparing \n",
      "\u001b[1B0ba8f8e0: Preparing \n",
      "\u001b[1B061c8df0: Preparing \n",
      "\u001b[1B20bdc5a8: Preparing \n",
      "\u001b[1B9623cc67: Preparing \n",
      "\u001b[1Bf5f47ef7: Preparing \n",
      "\u001b[1Be3a670db: Preparing \n",
      "\u001b[1Bab0a5210: Preparing \n",
      "\u001b[1B27b973c2: Preparing \n",
      "\u001b[1B003a8778: Preparing \n",
      "\u001b[1B6eb0eac1: Preparing \n",
      "\u001b[1Ba842d5cf: Preparing \n",
      "\u001b[1B7c820400: Preparing \n",
      "\u001b[1B2eabba29: Preparing \n",
      "\u001b[1B9e83e652: Preparing \n",
      "\u001b[1B8f4121e3: Preparing \n",
      "\u001b[1Bac953428: Preparing \n",
      "\u001b[1B89f48870: Preparing \n",
      "\u001b[1Bd7cd1026: Preparing \n",
      "\u001b[1Bf5c5948a: Preparing \n",
      "\u001b[1B06a133b8: Preparing \n",
      "\u001b[1B1c199f2f: Preparing \n",
      "\u001b[1Bd1f80fca: Preparing \n",
      "\u001b[1Bf0edb23d: Preparing \n",
      "\u001b[37B41a2243: Pushed   44.08MB/44.04MB\u001b[37A\u001b[2K\u001b[33A\u001b[2K\u001b[35A\u001b[2K\u001b[30A\u001b[2K\u001b[29A\u001b[2K\u001b[26A\u001b[2K\u001b[22A\u001b[2K\u001b[37A\u001b[2K\u001b[20A\u001b[2K\u001b[37A\u001b[2K\u001b[11A\u001b[2K\u001b[7A\u001b[2K\u001b[3A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2Klatest: digest: sha256:859f31e6783847eb66a47f17b6d19f1e74c3c52d5e4dbbb0cde6aeeaa6495e45 size: 8700\n"
     ]
    }
   ],
   "source": [
    "! docker push {TRAIN_IMAGE}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48d589c-4f86-4db6-bbf1-f60020a767d4",
   "metadata": {},
   "source": [
    "# Submitting training jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "39b83c1c-5a25-4071-bbcf-c7f9b4919273",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    staging_bucket=STAGING_BUCKET\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45602552-2e9a-42de-ac34-8e546bf9c388",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Hyperarameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705b3a8b-dddc-4742-967d-5426e813802d",
   "metadata": {},
   "source": [
    "### set Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c259c6a4-9e9f-477a-81da-ec0d7bf19122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT_NAME: jtv8-bert-hptune\n",
      "RUN_NAME: run-20230621-024011\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "EXPERIMENT_PREFIX = 'bert-hptune'\n",
    "EXPERIMENT_NAME=f'{PREFIX}-{EXPERIMENT_PREFIX}'\n",
    "RUN_NAME = f'run-{time.strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "\n",
    "print(f\"EXPERIMENT_NAME: {EXPERIMENT_NAME}\")\n",
    "print(f\"RUN_NAME: {RUN_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c408fcde-69d3-4e80-bd45-57af3f431cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MACHINE_TYPE          : n1-standard-16\n",
      "TRAIN_GPU             : NVIDIA_TESLA_T4\n",
      "TRAIN_NGPU            : 1\n",
      "REPLICA_COUNT         : 1\n",
      "DISTRIBUTION_STRATEGY : single\n",
      "HP_TUNING             : True\n"
     ]
    }
   ],
   "source": [
    "MACHINE_TYPE = 'n1-standard-16'\n",
    "TRAIN_GPU, TRAIN_NGPU = ('NVIDIA_TESLA_T4', 1) # NVIDIA_TESLA_T4 NVIDIA_TESLA_V100\n",
    "\n",
    "REPLICA_COUNT = 1\n",
    "DISTRIBUTION_STRATEGY = \"single\" # single, mirrored, multiworker, tpu\n",
    "\n",
    "HP_TUNING=\"True\"\n",
    "\n",
    "print(f\"MACHINE_TYPE          : {MACHINE_TYPE}\")\n",
    "print(f\"TRAIN_GPU             : {TRAIN_GPU}\")\n",
    "print(f\"TRAIN_NGPU            : {TRAIN_NGPU}\")\n",
    "print(f\"REPLICA_COUNT         : {REPLICA_COUNT}\")\n",
    "print(f\"DISTRIBUTION_STRATEGY : {DISTRIBUTION_STRATEGY}\")\n",
    "print(f\"HP_TUNING             : {HP_TUNING}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3c410ade-15df-40ea-8ab9-8df9041267b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'container_spec': {'args': ['--epochs=3',\n",
      "                              '--steps_per_epoch=200',\n",
      "                              '--eval_steps=50',\n",
      "                              '--per_replica_batch_size=32',\n",
      "                              '--training_data_path=gs://jtv8-hybrid-vertex-bucket/bert-finetuning/imdb/tfrecords/train',\n",
      "                              '--validation_data_path=gs://jtv8-hybrid-vertex-bucket/bert-finetuning/imdb/tfrecords/valid',\n",
      "                              '--testing_data_path=gs://jtv8-hybrid-vertex-bucket/bert-finetuning/imdb/tfrecords/test',\n",
      "                              '--job_dir=gs://jtv8-hybrid-vertex-bucket/jobs/job-20230621031158',\n",
      "                              '--strategy=single',\n",
      "                              '--auto_shard_policy=data',\n",
      "                              '--job_id=job-20230621031158',\n",
      "                              '--TRAIN_GPU=NVIDIA_TESLA_T4',\n",
      "                              '--TRAIN_NGPU=1',\n",
      "                              '--reduction_cnt=0',\n",
      "                              '--replica_count=1',\n",
      "                              '--experiment_name=jtv8-bert-hptune',\n",
      "                              '--experiment_run=run-20230621-024011',\n",
      "                              '--learning_rate=0.001',\n",
      "                              '--tuning=True'],\n",
      "                     'image_uri': 'gcr.io/hybrid-vertex/imdb_bert'},\n",
      "  'machine_spec': {'accelerator_count': 1,\n",
      "                   'accelerator_type': 'NVIDIA_TESLA_T4',\n",
      "                   'machine_type': 'n1-standard-16'},\n",
      "  'replica_count': 1}]\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "steps_per_epoch = 200\n",
    "eval_steps = 50\n",
    "\n",
    "PER_REPLICA_BATCH_SIZE = 32\n",
    "LEARNING_RATE= 0.001\n",
    "\n",
    "REDUCTION_SERVER_COUNT = 0\n",
    "REDUCTION_SERVER_MACHINE_TYPE = \"n1-highcpu-16\"\n",
    "\n",
    "training_data_path = f'{STAGING_BUCKET}/bert-finetuning/imdb/tfrecords/train'\n",
    "validation_data_path = f'{STAGING_BUCKET}/bert-finetuning/imdb/tfrecords/valid'\n",
    "testing_data_path = f'{STAGING_BUCKET}/bert-finetuning/imdb/tfrecords/test'\n",
    "\n",
    "job_id = f'job-{datetime.now().strftime(\"%Y%m%d%H%M%S\")}'.format()\n",
    "job_dir = f'{STAGING_BUCKET}/jobs/{job_id}'\n",
    "\n",
    "WORKER_ARGS = [\n",
    "    \"--epochs=\" + str(epochs)\n",
    "    , \"--steps_per_epoch=\" + str(steps_per_epoch)\n",
    "    , \"--eval_steps=\" + str(eval_steps)\n",
    "    , \"--per_replica_batch_size=\" + str(PER_REPLICA_BATCH_SIZE)\n",
    "    , \"--training_data_path=\" + training_data_path\n",
    "    , \"--validation_data_path=\" + validation_data_path\n",
    "    , \"--testing_data_path=\" + testing_data_path\n",
    "    , \"--job_dir=\" + job_dir\n",
    "    , f\"--strategy={DISTRIBUTION_STRATEGY}\"\n",
    "    , \"--auto_shard_policy=data\" #data\n",
    "    , f\"--job_id={job_id}\"\n",
    "    , f\"--TRAIN_GPU={TRAIN_GPU}\"\n",
    "    , f\"--TRAIN_NGPU={TRAIN_NGPU}\"\n",
    "    , f\"--reduction_cnt={REDUCTION_SERVER_COUNT}\"\n",
    "    , f\"--replica_count={REPLICA_COUNT}\"\n",
    "    , f\"--experiment_name={EXPERIMENT_NAME}\"\n",
    "    , f\"--experiment_run={RUN_NAME}\"\n",
    "    , f\"--learning_rate={LEARNING_RATE}\"\n",
    "    , f\"--tuning={HP_TUNING}\"\n",
    "]\n",
    "\n",
    "from utils import workerpool_specs\n",
    "\n",
    "WORKER_POOL_SPECS = workerpool_specs.prepare_worker_pool_specs(\n",
    "    image_uri=TRAIN_IMAGE,\n",
    "    args=WORKER_ARGS,\n",
    "    replica_count=REPLICA_COUNT,\n",
    "    machine_type=MACHINE_TYPE,\n",
    "    accelerator_count=TRAIN_NGPU,\n",
    "    accelerator_type=TRAIN_GPU,\n",
    "    reduction_server_count=REDUCTION_SERVER_COUNT,\n",
    "    reduction_server_machine_type=REDUCTION_SERVER_MACHINE_TYPE,\n",
    ")\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(WORKER_POOL_SPECS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d20100a-2b03-4e22-8b62-246a14f5c14c",
   "metadata": {},
   "source": [
    "Create a `CustomJob`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1b5439f3-68d0-4cc4-a74c-f5d249ca9540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CustomJob\n",
    "\n",
    "JOB_NAME = f'hptune-bert-{RUN_NAME}-{DISTRIBUTION_STRATEGY}-{REPLICA_COUNT}-{TRAIN_NGPU}'\n",
    "\n",
    "my_custom_hpt_job = vertex_ai.CustomJob(\n",
    "    display_name=JOB_NAME,\n",
    "    project=PROJECT_ID,\n",
    "    worker_pool_specs=WORKER_POOL_SPECS,\n",
    "    staging_bucket=f'{STAGING_BUCKET}/{EXPERIMENT_NAME}/{RUN_NAME}',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba80ab9-fb3e-41cd-a3ad-751c04a8f0fe",
   "metadata": {},
   "source": [
    "Once your container is pushed to Google Container Registry, you use the Vertex SDK to create and run the hyperparameter tuning job.\n",
    "\n",
    "You define the following specifications:\n",
    "\n",
    "* `parameter_spec`: Dictionary specifying the parameters to optimize. The dictionary key is the string assigned to the command line argument for each hyperparameter in your training application code, and the dictionary value is the parameter specification. The parameter specification includes the type, min/max values, and scale for the hyperparameter.\n",
    "\n",
    "* `metric_spec`: Dictionary specifying the metric to optimize. The dictionary key is the `hyperparameter_metric_tag` that you set in your training application code, and the value is the optimization goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a6da6394-eb82-4373-81c1-53f464ef255f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud.aiplatform import hyperparameter_tuning as hpt\n",
    "\n",
    "metric_spec = {\"binary_accuracy\": \"maximize\"}\n",
    "\n",
    "parameter_spec = {\n",
    "    \"learning_rate\": hpt.DoubleParameterSpec(min=0.001, max=1, scale=\"log\"),\n",
    "    \"per_replica_batch_size\": hpt.DiscreteParameterSpec(values=[32, 64, 128], scale=None),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d8c9f2-a557-4333-be17-5cb7f646969b",
   "metadata": {},
   "source": [
    "Then, create and run a HyperparameterTuningJob.\n",
    "\n",
    "There are a few arguments to note:\n",
    "\n",
    "* `max_trial_count`: Sets an upper bound on the number of trials the service will run. The recommended practice is to start with a smaller number of trials and get a sense of how impactful your chosen hyperparameters are before scaling up.\n",
    "\n",
    "* `parallel_trial_count`: If you use parallel trials, the service provisions multiple training processing clusters. The worker pool spec that you specify when creating the job is used for each individual training cluster. Increasing the number of parallel trials reduces the amount of time the hyperparameter tuning job takes to run; however, it can reduce the effectiveness of the job overall. This is because the default tuning strategy uses results of previous trials to inform the assignment of values in subsequent trials.\n",
    "\n",
    "* `search_algorithm`: The available search algorithms are grid, random, or default (None). The default option applies Bayesian optimization to search the space of possible hyperparameter values and is the recommended algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2d54dcb7-92b9-4a42-9a77-d0dd5374532e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and run HyperparameterTuningJob\n",
    "\n",
    "hp_job = vertex_ai.HyperparameterTuningJob(\n",
    "    display_name=JOB_NAME,\n",
    "    custom_job=my_custom_hpt_job,\n",
    "    metric_spec=metric_spec,\n",
    "    parameter_spec=parameter_spec,\n",
    "    max_trial_count=6,\n",
    "    parallel_trial_count=3,\n",
    "    project=PROJECT_ID,\n",
    "    search_algorithm=None,\n",
    ")\n",
    "\n",
    "hp_job.run(\n",
    "    sync=False\n",
    "    , service_account=VERTEX_SA\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "56abe121-e589-4b99-9f7d-8ed90fcaf6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Name: hptune-bert-run-20230621-024011-single-1-1\n",
      "Job Resource Name: projects/934903580331/locations/us-central1/hyperparameterTuningJobs/1775640772675108864\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Job Name: {hp_job.display_name}\")\n",
    "print(f\"Job Resource Name: {hp_job.resource_name}\\n\")\n",
    "# print(f\"Check training progress at {custom_job._dashboard_uri()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffee570-ab96-46ca-965a-7ad8fcba7922",
   "metadata": {},
   "source": [
    "### best trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df8ce6a-4acb-4269-8602-26a08f6f0afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "best = (None, None, None, 0.0)\n",
    "for trial in hpt_job.trials:\n",
    "    # Keep track of the best outcome\n",
    "    if float(trial.final_measurement.metrics[0].value) > best[3]:\n",
    "        try:\n",
    "            best = (\n",
    "                trial.id,\n",
    "                float(trial.parameters[0].value),\n",
    "                float(trial.parameters[1].value),\n",
    "                float(trial.final_measurement.metrics[0].value),\n",
    "            )\n",
    "        except:\n",
    "            best = (\n",
    "                trial.id,\n",
    "                float(trial.parameters[0].value),\n",
    "                None,\n",
    "                float(trial.final_measurement.metrics[0].value),\n",
    "            )\n",
    "\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d22551-884a-4dc3-80ed-7135dd880419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LR = best[2]\n",
    "# BATCH_SIZE = int(best[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f49f512-71c3-4cdc-bb2f-7ebf0186355b",
   "metadata": {},
   "source": [
    "## 1 Replica, 1 GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998250ad-48d0-4334-be4c-050fbab84914",
   "metadata": {},
   "source": [
    "### set Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c18b1b2f-156f-46e3-897f-f9727468d258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT_NAME: jtv9-bert-tune\n",
      "RUN_NAME: run-20230621-141557\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "EXPERIMENT_PREFIX = 'bert'\n",
    "EXPERIMENT_NAME=f'{PREFIX}-bert-tune'\n",
    "RUN_NAME = f'run-{time.strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "\n",
    "print(f\"EXPERIMENT_NAME: {EXPERIMENT_NAME}\")\n",
    "print(f\"RUN_NAME: {RUN_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32532bf-5531-4a2c-bde6-1b2dfa7227da",
   "metadata": {},
   "source": [
    "### config compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "79b4c3fa-7c2e-47c4-9e2b-0a6732b20c80",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MACHINE_TYPE          : a2-highgpu-1g\n",
      "TRAIN_GPU             : NVIDIA_TESLA_A100\n",
      "TRAIN_NGPU            : 1\n",
      "REPLICA_COUNT         : 1\n",
      "DISTRIBUTION_STRATEGY : single\n",
      "HP_TUNING             : False\n"
     ]
    }
   ],
   "source": [
    "# MACHINE_TYPE = 'n1-standard-16'\n",
    "# TRAIN_GPU, TRAIN_NGPU = ('NVIDIA_TESLA_T4', 1)\n",
    "\n",
    "# MACHINE_TYPE = 'n1-standard-16'\n",
    "# TRAIN_GPU, TRAIN_NGPU = ('NVIDIA_TESLA_V100', 1)\n",
    "\n",
    "MACHINE_TYPE = 'a2-highgpu-1g'\n",
    "TRAIN_GPU, TRAIN_NGPU = ('NVIDIA_TESLA_A100', 1)\n",
    "\n",
    "# MACHINE_TYPE = 'a2-ultragpu-1g'\n",
    "# TRAIN_GPU, TRAIN_NGPU = ('NVIDIA_A100_80GB', 1)\n",
    "\n",
    "REPLICA_COUNT = 1\n",
    "DISTRIBUTION_STRATEGY = \"single\" # single, mirrored, multiworker, tpu\n",
    "\n",
    "HP_TUNING=\"False\"\n",
    "\n",
    "print(f\"MACHINE_TYPE          : {MACHINE_TYPE}\")\n",
    "print(f\"TRAIN_GPU             : {TRAIN_GPU}\")\n",
    "print(f\"TRAIN_NGPU            : {TRAIN_NGPU}\")\n",
    "print(f\"REPLICA_COUNT         : {REPLICA_COUNT}\")\n",
    "print(f\"DISTRIBUTION_STRATEGY : {DISTRIBUTION_STRATEGY}\")\n",
    "print(f\"HP_TUNING             : {HP_TUNING}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d736bb-770c-4843-9aed-b8246c8dd8e5",
   "metadata": {},
   "source": [
    "### worker args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5b5362d2-6f4e-49d7-9d6a-9a548bfe722d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'container_spec': {'args': ['--epochs=100',\n",
      "                              '--steps_per_epoch=200',\n",
      "                              '--eval_steps=50',\n",
      "                              '--per_replica_batch_size=32',\n",
      "                              '--training_data_path=gs://jtv9-hybrid-vertex-bucket/bert-finetuning/imdb/tfrecords/train',\n",
      "                              '--validation_data_path=gs://jtv9-hybrid-vertex-bucket/bert-finetuning/imdb/tfrecords/valid',\n",
      "                              '--testing_data_path=gs://jtv9-hybrid-vertex-bucket/bert-finetuning/imdb/tfrecords/test',\n",
      "                              '--job_dir=gs://jtv9-hybrid-vertex-bucket/jobs/job-20230621141630',\n",
      "                              '--strategy=single',\n",
      "                              '--auto_shard_policy=data',\n",
      "                              '--job_id=job-20230621141630',\n",
      "                              '--TRAIN_GPU=NVIDIA_TESLA_A100',\n",
      "                              '--TRAIN_NGPU=1',\n",
      "                              '--reduction_cnt=0',\n",
      "                              '--replica_count=1',\n",
      "                              '--experiment_name=jtv9-bert-tune',\n",
      "                              '--experiment_run=run-20230621-141557',\n",
      "                              '--learning_rate=0.001',\n",
      "                              '--tuning=False'],\n",
      "                     'image_uri': 'gcr.io/hybrid-vertex/imdb_bert'},\n",
      "  'machine_spec': {'accelerator_count': 1,\n",
      "                   'accelerator_type': 'NVIDIA_TESLA_A100',\n",
      "                   'machine_type': 'a2-highgpu-1g'},\n",
      "  'replica_count': 1}]\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "steps_per_epoch = 200\n",
    "eval_steps = 50\n",
    "\n",
    "PER_REPLICA_BATCH_SIZE = 32\n",
    "LEARNING_RATE= 0.001\n",
    "\n",
    "REDUCTION_SERVER_COUNT = 0\n",
    "REDUCTION_SERVER_MACHINE_TYPE = \"n1-highcpu-16\"\n",
    "\n",
    "training_data_path = f'{STAGING_BUCKET}/bert-finetuning/imdb/tfrecords/train'\n",
    "validation_data_path = f'{STAGING_BUCKET}/bert-finetuning/imdb/tfrecords/valid'\n",
    "testing_data_path = f'{STAGING_BUCKET}/bert-finetuning/imdb/tfrecords/test'\n",
    "\n",
    "job_id = f'job-{datetime.now().strftime(\"%Y%m%d%H%M%S\")}'.format()\n",
    "job_dir = f'{STAGING_BUCKET}/jobs/{job_id}'\n",
    "\n",
    "WORKER_ARGS = [\n",
    "    \"--epochs=\" + str(epochs)\n",
    "    , \"--steps_per_epoch=\" + str(steps_per_epoch)\n",
    "    , \"--eval_steps=\" + str(eval_steps)\n",
    "    , \"--per_replica_batch_size=\" + str(PER_REPLICA_BATCH_SIZE)\n",
    "    , \"--training_data_path=\" + training_data_path\n",
    "    , \"--validation_data_path=\" + validation_data_path\n",
    "    , \"--testing_data_path=\" + testing_data_path\n",
    "    , \"--job_dir=\" + job_dir\n",
    "    , f\"--strategy={DISTRIBUTION_STRATEGY}\"\n",
    "    , \"--auto_shard_policy=data\" # data | auto\n",
    "    , f\"--job_id={job_id}\"\n",
    "    , f\"--TRAIN_GPU={TRAIN_GPU}\"\n",
    "    , f\"--TRAIN_NGPU={TRAIN_NGPU}\"\n",
    "    , f\"--reduction_cnt={REDUCTION_SERVER_COUNT}\"\n",
    "    , f\"--replica_count={REPLICA_COUNT}\"\n",
    "    , f\"--experiment_name={EXPERIMENT_NAME}\"\n",
    "    , f\"--experiment_run={RUN_NAME}\"\n",
    "    , f\"--learning_rate={LEARNING_RATE}\"\n",
    "    , f\"--tuning={HP_TUNING}\"\n",
    "]\n",
    "\n",
    "from utils import workerpool_specs\n",
    "\n",
    "WORKER_POOL_SPECS = workerpool_specs.prepare_worker_pool_specs(\n",
    "    image_uri=TRAIN_IMAGE,\n",
    "    args=WORKER_ARGS,\n",
    "    replica_count=REPLICA_COUNT,\n",
    "    machine_type=MACHINE_TYPE,\n",
    "    accelerator_count=TRAIN_NGPU,\n",
    "    accelerator_type=TRAIN_GPU,\n",
    "    reduction_server_count=REDUCTION_SERVER_COUNT,\n",
    "    reduction_server_machine_type=REDUCTION_SERVER_MACHINE_TYPE,\n",
    ")\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(WORKER_POOL_SPECS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8f79af-f337-4153-a40e-49d799738000",
   "metadata": {},
   "source": [
    "### create tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9bfbe0a9-3d5c-4895-9039-55858aa2c83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "projects/934903580331/locations/us-central1/tensorboards/8961837803025465344\n"
     ]
    }
   ],
   "source": [
    "vertex_ai_tb = vertex_ai.Tensorboard.create()\n",
    "TENSORBOARD = vertex_ai_tb.gca_resource.name\n",
    "\n",
    "# use existing\n",
    "# TENSORBOARD=\"projects/934903580331/locations/us-central1/tensorboards/949934065933352960\"\n",
    "\n",
    "print(TENSORBOARD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "19c24174-dec7-4d12-a24a-cee5edbe8fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.init(\n",
    "    experiment=EXPERIMENT_NAME\n",
    "    # , experiment_tensorboard=vertex_ai_tb\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e016fad8-67ba-483e-a421-c40d4e9aeb54",
   "metadata": {},
   "source": [
    "### submit train job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c7ae09da-b908-4386-af8d-5050fc6afc1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tesla-a100\n"
     ]
    }
   ],
   "source": [
    "ACCELERATOR = TRAIN_GPU.lower().replace(\"nvidia_\",\"\").replace(\"_\",\"-\")\n",
    "print(ACCELERATOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c740910e-f137-4d82-852c-f12a9df3d281",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_job = vertex_ai.CustomJob(\n",
    "    display_name=f'imdb-bert-{DISTRIBUTION_STRATEGY}-{TRAIN_NGPU}-{ACCELERATOR}'\n",
    "    , worker_pool_specs=WORKER_POOL_SPECS\n",
    "    , staging_bucket=f'{STAGING_BUCKET}/{EXPERIMENT_NAME}/{RUN_NAME}'\n",
    "    # , location=REGION\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5e98565e-12df-42bc-a37b-fd1669453b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_job.run(\n",
    "    sync=False\n",
    "    , service_account=VERTEX_SA\n",
    "    , tensorboard=TENSORBOARD\n",
    "    , restart_job_on_worker_restart=False\n",
    "    , enable_web_access=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b889e3b7-0924-4a95-9db2-a600fc8fddc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Name: imdb-bert-single-1-tesla-a100\n",
      "Job Resource Name: projects/934903580331/locations/us-central1/customJobs/8584063062468722688\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Job Name: {custom_job.display_name}\")\n",
    "print(f\"Job Resource Name: {custom_job.resource_name}\\n\")\n",
    "# print(f\"Check training progress at {custom_job._dashboard_uri()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7585ba6c-ce57-408e-abaf-c4e77b9a8493",
   "metadata": {},
   "source": [
    "## 1 Replica, 2 GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d6bc57-446c-44fc-881e-b80ec2a4d00a",
   "metadata": {},
   "source": [
    "### set Experiment Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7d2d5f13-cec0-466a-8cbd-a43999f18777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT_NAME: jtv9-bert-tune\n",
      "RUN_NAME: run-20230621-155304\n"
     ]
    }
   ],
   "source": [
    "RUN_NAME = f'run-{time.strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "\n",
    "print(f\"EXPERIMENT_NAME: {EXPERIMENT_NAME}\")\n",
    "print(f\"RUN_NAME: {RUN_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2792df0d-673c-45d5-b50c-ff2f0f1e5f1f",
   "metadata": {},
   "source": [
    "### config compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6e7cd69d-9eaf-421e-8569-872c1cfab657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MACHINE_TYPE          : a2-highgpu-2g\n",
      "TRAIN_GPU             : NVIDIA_TESLA_A100\n",
      "TRAIN_NGPU            : 2\n",
      "REPLICA_COUNT         : 1\n",
      "DISTRIBUTION_STRATEGY : mirrored\n",
      "HP_TUNING             : False\n"
     ]
    }
   ],
   "source": [
    "# MACHINE_TYPE = 'n1-standard-32'\n",
    "# TRAIN_GPU, TRAIN_NGPU = ('NVIDIA_TESLA_T4', 2)\n",
    "\n",
    "# MACHINE_TYPE = 'n1-standard-16'\n",
    "# TRAIN_GPU, TRAIN_NGPU = ('NVIDIA_TESLA_V100', 2)\n",
    "\n",
    "MACHINE_TYPE = 'a2-highgpu-2g'\n",
    "TRAIN_GPU, TRAIN_NGPU = ('NVIDIA_TESLA_A100', 2)\n",
    "\n",
    "# MACHINE_TYPE = 'a2-ultragpu-2g'\n",
    "# TRAIN_GPU, TRAIN_NGPU = ('NVIDIA_A100_80GB', 2)\n",
    "\n",
    "REPLICA_COUNT = 1\n",
    "DISTRIBUTION_STRATEGY = \"mirrored\" # single, mirrored, multiworker, tpu\n",
    "\n",
    "HP_TUNING=\"False\"\n",
    "\n",
    "print(f\"MACHINE_TYPE          : {MACHINE_TYPE}\")\n",
    "print(f\"TRAIN_GPU             : {TRAIN_GPU}\")\n",
    "print(f\"TRAIN_NGPU            : {TRAIN_NGPU}\")\n",
    "print(f\"REPLICA_COUNT         : {REPLICA_COUNT}\")\n",
    "print(f\"DISTRIBUTION_STRATEGY : {DISTRIBUTION_STRATEGY}\")\n",
    "print(f\"HP_TUNING             : {HP_TUNING}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52049d95-f67c-445b-802a-4b930d3bdb5c",
   "metadata": {},
   "source": [
    "### worker args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "87ac4831-19e0-4819-a301-dfc33c088baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'container_spec': {'args': ['--epochs=10',\n",
      "                              '--steps_per_epoch=200',\n",
      "                              '--eval_steps=50',\n",
      "                              '--per_replica_batch_size=32',\n",
      "                              '--training_data_path=gs://jtv9-hybrid-vertex-bucket/bert-finetuning/imdb/tfrecords/train',\n",
      "                              '--validation_data_path=gs://jtv9-hybrid-vertex-bucket/bert-finetuning/imdb/tfrecords/valid',\n",
      "                              '--testing_data_path=gs://jtv9-hybrid-vertex-bucket/bert-finetuning/imdb/tfrecords/test',\n",
      "                              '--job_dir=gs://jtv9-hybrid-vertex-bucket/jobs/job-20230621155321',\n",
      "                              '--strategy=mirrored',\n",
      "                              '--auto_shard_policy=auto',\n",
      "                              '--job_id=job-20230621155321',\n",
      "                              '--TRAIN_GPU=NVIDIA_TESLA_A100',\n",
      "                              '--TRAIN_NGPU=2',\n",
      "                              '--reduction_cnt=0',\n",
      "                              '--replica_count=1',\n",
      "                              '--experiment_name=jtv9-bert-tune',\n",
      "                              '--experiment_run=run-20230621-155304',\n",
      "                              '--learning_rate=0.001',\n",
      "                              '--tuning=False'],\n",
      "                     'image_uri': 'gcr.io/hybrid-vertex/imdb_bert'},\n",
      "  'machine_spec': {'accelerator_count': 2,\n",
      "                   'accelerator_type': 'NVIDIA_TESLA_A100',\n",
      "                   'machine_type': 'a2-highgpu-2g'},\n",
      "  'replica_count': 1}]\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "steps_per_epoch = 200\n",
    "eval_steps = 50\n",
    "\n",
    "PER_REPLICA_BATCH_SIZE = 32\n",
    "LEARNING_RATE= 0.001\n",
    "\n",
    "REDUCTION_SERVER_COUNT = 0\n",
    "REDUCTION_SERVER_MACHINE_TYPE = \"n1-highcpu-16\"\n",
    "\n",
    "training_data_path = f'{STAGING_BUCKET}/bert-finetuning/imdb/tfrecords/train'\n",
    "validation_data_path = f'{STAGING_BUCKET}/bert-finetuning/imdb/tfrecords/valid'\n",
    "testing_data_path = f'{STAGING_BUCKET}/bert-finetuning/imdb/tfrecords/test'\n",
    "\n",
    "job_id = f'job-{datetime.now().strftime(\"%Y%m%d%H%M%S\")}'.format()\n",
    "job_dir = f'{STAGING_BUCKET}/jobs/{job_id}'\n",
    "\n",
    "WORKER_ARGS = [\n",
    "    \"--epochs=\" + str(epochs)\n",
    "    , \"--steps_per_epoch=\" + str(steps_per_epoch)\n",
    "    , \"--eval_steps=\" + str(eval_steps)\n",
    "    , \"--per_replica_batch_size=\" + str(PER_REPLICA_BATCH_SIZE)\n",
    "    , \"--training_data_path=\" + training_data_path\n",
    "    , \"--validation_data_path=\" + validation_data_path\n",
    "    , \"--testing_data_path=\" + testing_data_path\n",
    "    , \"--job_dir=\" + job_dir\n",
    "    , f\"--strategy={DISTRIBUTION_STRATEGY}\"\n",
    "    , \"--auto_shard_policy=auto\" #data\n",
    "    , f\"--job_id={job_id}\"\n",
    "    , f\"--TRAIN_GPU={TRAIN_GPU}\"\n",
    "    , f\"--TRAIN_NGPU={TRAIN_NGPU}\"\n",
    "    , f\"--reduction_cnt={REDUCTION_SERVER_COUNT}\"\n",
    "    , f\"--replica_count={REPLICA_COUNT}\"\n",
    "    , f\"--experiment_name={EXPERIMENT_NAME}\"\n",
    "    , f\"--experiment_run={RUN_NAME}\"\n",
    "    , f\"--learning_rate={LEARNING_RATE}\"\n",
    "    , f\"--tuning={HP_TUNING}\"\n",
    "]\n",
    "\n",
    "from utils import workerpool_specs\n",
    "\n",
    "WORKER_POOL_SPECS = workerpool_specs.prepare_worker_pool_specs(\n",
    "    image_uri=TRAIN_IMAGE,\n",
    "    args=WORKER_ARGS,\n",
    "    replica_count=REPLICA_COUNT,\n",
    "    machine_type=MACHINE_TYPE,\n",
    "    accelerator_count=TRAIN_NGPU,\n",
    "    accelerator_type=TRAIN_GPU,\n",
    "    reduction_server_count=REDUCTION_SERVER_COUNT,\n",
    "    reduction_server_machine_type=REDUCTION_SERVER_MACHINE_TYPE,\n",
    ")\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(WORKER_POOL_SPECS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd3d0ee-23ae-4be9-8ce9-c3770e2c448f",
   "metadata": {},
   "source": [
    "### create tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c7ff735b-fa86-424b-88a4-219c3d779ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "projects/934903580331/locations/us-central1/tensorboards/9128470989238173696\n"
     ]
    }
   ],
   "source": [
    "vertex_ai_tb = vertex_ai.Tensorboard.create()\n",
    "TENSORBOARD = vertex_ai_tb.gca_resource.name\n",
    "\n",
    "# TENSORBOARD=\"projects/934903580331/locations/us-central1/tensorboards/6822627980024479744\"\n",
    "\n",
    "print(TENSORBOARD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a80ea24b-d7a5-4d1f-ad60-3691a56014ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.init(\n",
    "    experiment=EXPERIMENT_NAME\n",
    "    # , experiment_tensorboard=vertex_ai_tb\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c724bc34-f467-420d-8c02-b98b69bfdbb4",
   "metadata": {},
   "source": [
    "### submit train job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6f12c74b-bfb4-42a9-9f21-d3f5cac4753e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tesla-a100\n"
     ]
    }
   ],
   "source": [
    "ACCELERATOR = TRAIN_GPU.lower().replace(\"nvidia_\",\"\").replace(\"_\",\"-\")\n",
    "print(ACCELERATOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0453f249-a24e-42a5-aa08-7bfe011e2dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_m_job = vertex_ai.CustomJob(\n",
    "    display_name=f'imdb-bert-{DISTRIBUTION_STRATEGY}-{TRAIN_NGPU}-{ACCELERATOR}',\n",
    "    worker_pool_specs=WORKER_POOL_SPECS,\n",
    "    staging_bucket=f'{STAGING_BUCKET}/{EXPERIMENT_NAME}/{RUN_NAME}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a79b3764-85e8-462b-997b-89c29040785d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "custom_m_job.run(\n",
    "    sync=False\n",
    "    , service_account=VERTEX_SA\n",
    "    , tensorboard=TENSORBOARD\n",
    "    , restart_job_on_worker_restart=False\n",
    "    , enable_web_access=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "40c56959-bda6-4998-8e2d-a832ff1880c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Name: imdb-bert-mirrored-2-tesla-a100\n",
      "Job Resource Name: projects/934903580331/locations/us-central1/customJobs/5587198985430368256\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Job Name: {custom_m_job.display_name}\")\n",
    "print(f\"Job Resource Name: {custom_m_job.resource_name}\\n\")\n",
    "# print(f\"Check training progress at {custom_job._dashboard_uri()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab6abfa-2808-4cdb-8171-9f70274c44b0",
   "metadata": {},
   "source": [
    "## 2 Replicas, 1 GPU each"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a58ba5-85b0-4408-9178-bfd110a1115f",
   "metadata": {},
   "source": [
    "> Now increase `replica_count` from 1 to 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b687e5d6-5287-46f4-9347-ef5ad14df96f",
   "metadata": {},
   "source": [
    "### set Experiment Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a0e3616b-db10-4d88-84c2-e71ede086ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT_NAME: jtv7-bert-tune\n",
      "RUN_NAME: run-20230621-012310\n"
     ]
    }
   ],
   "source": [
    "RUN_NAME = f'run-{time.strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "\n",
    "print(f\"EXPERIMENT_NAME: {EXPERIMENT_NAME}\")\n",
    "print(f\"RUN_NAME: {RUN_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5187c57-76e9-4c49-94d4-0fca38fff14f",
   "metadata": {},
   "source": [
    "### config compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "19cccf96-015a-4bde-a3d4-9ecc316b7700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MACHINE_TYPE          : n1-standard-16\n",
      "TRAIN_GPU             : NVIDIA_TESLA_T4\n",
      "TRAIN_NGPU            : 1\n",
      "REPLICA_COUNT         : 2\n",
      "DISTRIBUTION_STRATEGY : multiworker\n"
     ]
    }
   ],
   "source": [
    "MACHINE_TYPE = 'n1-standard-16'\n",
    "TRAIN_GPU, TRAIN_NGPU = ('NVIDIA_TESLA_T4', 1) # NVIDIA_TESLA_T4 NVIDIA_TESLA_V100\n",
    "\n",
    "REPLICA_COUNT = 2\n",
    "DISTRIBUTION_STRATEGY = \"multiworker\" # single, mirrored, multiworker, tpu\n",
    "\n",
    "HP_TUNING=\"False\"\n",
    "\n",
    "print(f\"MACHINE_TYPE          : {MACHINE_TYPE}\")\n",
    "print(f\"TRAIN_GPU             : {TRAIN_GPU}\")\n",
    "print(f\"TRAIN_NGPU            : {TRAIN_NGPU}\")\n",
    "print(f\"REPLICA_COUNT         : {REPLICA_COUNT}\")\n",
    "print(f\"DISTRIBUTION_STRATEGY : {DISTRIBUTION_STRATEGY}\")\n",
    "print(f\"HP_TUNING             : {HP_TUNING}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a3ac22-1eff-46ab-ac22-1b62f8a6d6ec",
   "metadata": {},
   "source": [
    "### worker args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "726e5b05-102e-4f3c-be16-a025cbb452ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'container_spec': {'args': ['--epochs=10',\n",
      "                              '--steps_per_epoch=200',\n",
      "                              '--eval_steps=50',\n",
      "                              '--per_replica_batch_size=32',\n",
      "                              '--training_data_path=gs://jtv7-hybrid-vertex-bucket/bert-finetuning/imdb/tfrecords/train',\n",
      "                              '--validation_data_path=gs://jtv7-hybrid-vertex-bucket/bert-finetuning/imdb/tfrecords/valid',\n",
      "                              '--testing_data_path=gs://jtv7-hybrid-vertex-bucket/bert-finetuning/imdb/tfrecords/test',\n",
      "                              '--job_dir=gs://jtv7-hybrid-vertex-bucket/jobs/job-20230621012317',\n",
      "                              '--strategy=multiworker',\n",
      "                              '--auto_shard_policy=data',\n",
      "                              '--job_id=job-20230621012317',\n",
      "                              '--TRAIN_GPU=NVIDIA_TESLA_T4',\n",
      "                              '--TRAIN_NGPU=1',\n",
      "                              '--reduction_cnt=0',\n",
      "                              '--replica_count=2',\n",
      "                              '--experiment_name=jtv7-bert-tune',\n",
      "                              '--experiment_run=run-20230621-012310'],\n",
      "                     'image_uri': 'gcr.io/hybrid-vertex/imdb_bert'},\n",
      "  'machine_spec': {'accelerator_count': 1,\n",
      "                   'accelerator_type': 'NVIDIA_TESLA_T4',\n",
      "                   'machine_type': 'n1-standard-16'},\n",
      "  'replica_count': 1},\n",
      " {'container_spec': {'args': ['--epochs=10',\n",
      "                              '--steps_per_epoch=200',\n",
      "                              '--eval_steps=50',\n",
      "                              '--per_replica_batch_size=32',\n",
      "                              '--training_data_path=gs://jtv7-hybrid-vertex-bucket/bert-finetuning/imdb/tfrecords/train',\n",
      "                              '--validation_data_path=gs://jtv7-hybrid-vertex-bucket/bert-finetuning/imdb/tfrecords/valid',\n",
      "                              '--testing_data_path=gs://jtv7-hybrid-vertex-bucket/bert-finetuning/imdb/tfrecords/test',\n",
      "                              '--job_dir=gs://jtv7-hybrid-vertex-bucket/jobs/job-20230621012317',\n",
      "                              '--strategy=multiworker',\n",
      "                              '--auto_shard_policy=data',\n",
      "                              '--job_id=job-20230621012317',\n",
      "                              '--TRAIN_GPU=NVIDIA_TESLA_T4',\n",
      "                              '--TRAIN_NGPU=1',\n",
      "                              '--reduction_cnt=0',\n",
      "                              '--replica_count=2',\n",
      "                              '--experiment_name=jtv7-bert-tune',\n",
      "                              '--experiment_run=run-20230621-012310'],\n",
      "                     'image_uri': 'gcr.io/hybrid-vertex/imdb_bert'},\n",
      "  'machine_spec': {'accelerator_count': 1,\n",
      "                   'accelerator_type': 'NVIDIA_TESLA_T4',\n",
      "                   'machine_type': 'n1-standard-16'},\n",
      "  'replica_count': 1}]\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "steps_per_epoch = 200\n",
    "eval_steps = 50\n",
    "\n",
    "PER_REPLICA_BATCH_SIZE = 32\n",
    "LEARNING_RATE= 0.001\n",
    "\n",
    "REDUCTION_SERVER_COUNT = 0\n",
    "REDUCTION_SERVER_MACHINE_TYPE = \"n1-highcpu-16\"\n",
    "\n",
    "training_data_path = f'{STAGING_BUCKET}/bert-finetuning/imdb/tfrecords/train'\n",
    "validation_data_path = f'{STAGING_BUCKET}/bert-finetuning/imdb/tfrecords/valid'\n",
    "testing_data_path = f'{STAGING_BUCKET}/bert-finetuning/imdb/tfrecords/test'\n",
    "job_id = 'job-{}'.format(datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
    "job_dir = f'{STAGING_BUCKET}/jobs/{job_id}'\n",
    "\n",
    "WORKER_ARGS = [\n",
    "    \"--epochs=\" + str(epochs)\n",
    "    , \"--steps_per_epoch=\" + str(steps_per_epoch)\n",
    "    , \"--eval_steps=\" + str(eval_steps)\n",
    "    , \"--per_replica_batch_size=\" + str(PER_REPLICA_BATCH_SIZE)\n",
    "    , \"--training_data_path=\" + training_data_path\n",
    "    , \"--validation_data_path=\" + validation_data_path\n",
    "    , \"--testing_data_path=\" + testing_data_path\n",
    "    , \"--job_dir=\" + job_dir\n",
    "    , f\"--strategy={DISTRIBUTION_STRATEGY}\"\n",
    "    , \"--auto_shard_policy=data\" # data auto\n",
    "    , f\"--job_id={job_id}\"\n",
    "    , f\"--TRAIN_GPU={TRAIN_GPU}\"\n",
    "    , f\"--TRAIN_NGPU={TRAIN_NGPU}\"\n",
    "    , f\"--reduction_cnt={REDUCTION_SERVER_COUNT}\"\n",
    "    , f\"--replica_count={REPLICA_COUNT}\"\n",
    "    , f\"--experiment_name={EXPERIMENT_NAME}\"\n",
    "    , f\"--experiment_run={RUN_NAME}\"\n",
    "    , f\"--learning_rate={LEARNING_RATE}\"\n",
    "    , f\"--tuning={HP_TUNING}\"\n",
    "]\n",
    "\n",
    "from utils import workerpool_specs\n",
    "\n",
    "WORKER_POOL_SPECS = workerpool_specs.prepare_worker_pool_specs(\n",
    "    image_uri=TRAIN_IMAGE,\n",
    "    args=WORKER_ARGS,\n",
    "    replica_count=REPLICA_COUNT,\n",
    "    machine_type=MACHINE_TYPE,\n",
    "    accelerator_count=TRAIN_NGPU,\n",
    "    accelerator_type=TRAIN_GPU,\n",
    "    reduction_server_count=REDUCTION_SERVER_COUNT,\n",
    "    reduction_server_machine_type=REDUCTION_SERVER_MACHINE_TYPE,\n",
    ")\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(WORKER_POOL_SPECS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b23d74-06d1-4052-bc27-418acc6eb0ca",
   "metadata": {},
   "source": [
    "### create tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "30345523-3c20-473f-85a4-4a61963c2072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "projects/934903580331/locations/us-central1/tensorboards/4467245374909710336\n"
     ]
    }
   ],
   "source": [
    "vertex_ai_tb = vertex_ai.Tensorboard.create()\n",
    "TENSORBOARD = vertex_ai_tb.gca_resource.name\n",
    "\n",
    "# TENSORBOARD=\"projects/934903580331/locations/us-central1/tensorboards/4467245374909710336\"\n",
    "\n",
    "print(TENSORBOARD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "89db16c0-0027-4fb2-accb-3433add133e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.init(\n",
    "    experiment=EXPERIMENT_NAME\n",
    "    # , experiment_tensorboard=vertex_ai_tb\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc527ed3-48f2-4c4b-9325-03806b3fd10e",
   "metadata": {},
   "source": [
    "### submit train job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f586cea3-7a4c-4eb7-8761-63c8979fb66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCELERATOR = TRAIN_GPU.lower().replace(\"nvidia_\",\"\").replace(\"_\",\"-\")\n",
    "print(ACCELERATOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3386f043-bf34-4a9e-b20a-3c124a7be353",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_mm_job = vertex_ai.CustomJob(\n",
    "    display_name=f'imdb-bert-{DISTRIBUTION_STRATEGY}-{TRAIN_NGPU}-{ACCELERATOR}',\n",
    "    worker_pool_specs=WORKER_POOL_SPECS,\n",
    "    staging_bucket=f'{STAGING_BUCKET}/{EXPERIMENT_NAME}/{RUN_NAME}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a2e33d75-c508-420c-a295-1ff5f7ed95e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "custom_mm_job.run(\n",
    "    sync=False\n",
    "    , service_account=VERTEX_SA\n",
    "    , tensorboard=TENSORBOARD\n",
    "    , restart_job_on_worker_restart=False\n",
    "    , enable_web_access=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c2a452a2-90fd-48c5-bd88-6ddf2f308911",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Name: imdb-bert-run-20230621-012310-multiworker-2-1\n",
      "Job Resource Name: projects/934903580331/locations/us-central1/customJobs/3847648444986425344\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Job Name: {custom_mm_job.display_name}\")\n",
    "print(f\"Job Resource Name: {custom_mm_job.resource_name}\\n\")\n",
    "# print(f\"Check training progress at {custom_m_job._dashboard_uri()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68bf1aa-7a2e-4e5c-90a4-4810382c6072",
   "metadata": {},
   "source": [
    "## 2 Replicas, 1 GPU each + Reduction Server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41022c94-5805-4e8d-948e-c09177ad50cb",
   "metadata": {},
   "source": [
    "### set Experiment Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a2ba2172-1102-413a-b541-b9e23e757da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT_NAME: jtv7-bert-tune\n",
      "RUN_NAME: run-20230621-012602\n"
     ]
    }
   ],
   "source": [
    "RUN_NAME = f'run-{time.strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "\n",
    "print(f\"EXPERIMENT_NAME: {EXPERIMENT_NAME}\")\n",
    "print(f\"RUN_NAME: {RUN_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fd8239-b9ff-4eeb-9536-a6fba4e785d1",
   "metadata": {},
   "source": [
    "### config compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2a7ff1cb-8554-45d3-a3fb-6ec46a797630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MACHINE_TYPE          : n1-standard-16\n",
      "TRAIN_GPU             : NVIDIA_TESLA_T4\n",
      "TRAIN_NGPU            : 1\n",
      "REPLICA_COUNT         : 2\n",
      "DISTRIBUTION_STRATEGY : multiworker\n"
     ]
    }
   ],
   "source": [
    "MACHINE_TYPE = 'n1-standard-16'\n",
    "TRAIN_GPU, TRAIN_NGPU = ('NVIDIA_TESLA_T4', 1) # NVIDIA_TESLA_T4 NVIDIA_TESLA_V100\n",
    "\n",
    "REPLICA_COUNT = 2\n",
    "DISTRIBUTION_STRATEGY = \"multiworker\" # single, mirrored, multiworker, tpu\n",
    "\n",
    "HP_TUNING=\"False\"\n",
    "\n",
    "print(f\"MACHINE_TYPE          : {MACHINE_TYPE}\")\n",
    "print(f\"TRAIN_GPU             : {TRAIN_GPU}\")\n",
    "print(f\"TRAIN_NGPU            : {TRAIN_NGPU}\")\n",
    "print(f\"REPLICA_COUNT         : {REPLICA_COUNT}\")\n",
    "print(f\"DISTRIBUTION_STRATEGY : {DISTRIBUTION_STRATEGY}\")\n",
    "print(f\"HP_TUNING             : {HP_TUNING}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6d6206-7400-49b5-a057-51c234a455e2",
   "metadata": {},
   "source": [
    "### worker args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6222a212-5d29-4036-b07b-a9c4cb33f073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'container_spec': {'args': ['--epochs=10',\n",
      "                              '--steps_per_epoch=200',\n",
      "                              '--eval_steps=50',\n",
      "                              '--per_replica_batch_size=32',\n",
      "                              '--training_data_path=gs://jtv7-hybrid-vertex-bucket/bert-finetuning/imdb/tfrecords/train',\n",
      "                              '--validation_data_path=gs://jtv7-hybrid-vertex-bucket/bert-finetuning/imdb/tfrecords/valid',\n",
      "                              '--testing_data_path=gs://jtv7-hybrid-vertex-bucket/bert-finetuning/imdb/tfrecords/test',\n",
      "                              '--job_dir=gs://jtv7-hybrid-vertex-bucket/jobs/job-20230621012604',\n",
      "                              '--strategy=multiworker',\n",
      "                              '--auto_shard_policy=data',\n",
      "                              '--job_id=job-20230621012604',\n",
      "                              '--TRAIN_GPU=NVIDIA_TESLA_T4',\n",
      "                              '--TRAIN_NGPU=1',\n",
      "                              '--reduction_cnt=2',\n",
      "                              '--replica_count=2',\n",
      "                              '--experiment_name=jtv7-bert-tune',\n",
      "                              '--experiment_run=run-20230621-012602'],\n",
      "                     'image_uri': 'gcr.io/hybrid-vertex/imdb_bert'},\n",
      "  'machine_spec': {'accelerator_count': 1,\n",
      "                   'accelerator_type': 'NVIDIA_TESLA_T4',\n",
      "                   'machine_type': 'n1-standard-16'},\n",
      "  'replica_count': 1},\n",
      " {'container_spec': {'args': ['--epochs=10',\n",
      "                              '--steps_per_epoch=200',\n",
      "                              '--eval_steps=50',\n",
      "                              '--per_replica_batch_size=32',\n",
      "                              '--training_data_path=gs://jtv7-hybrid-vertex-bucket/bert-finetuning/imdb/tfrecords/train',\n",
      "                              '--validation_data_path=gs://jtv7-hybrid-vertex-bucket/bert-finetuning/imdb/tfrecords/valid',\n",
      "                              '--testing_data_path=gs://jtv7-hybrid-vertex-bucket/bert-finetuning/imdb/tfrecords/test',\n",
      "                              '--job_dir=gs://jtv7-hybrid-vertex-bucket/jobs/job-20230621012604',\n",
      "                              '--strategy=multiworker',\n",
      "                              '--auto_shard_policy=data',\n",
      "                              '--job_id=job-20230621012604',\n",
      "                              '--TRAIN_GPU=NVIDIA_TESLA_T4',\n",
      "                              '--TRAIN_NGPU=1',\n",
      "                              '--reduction_cnt=2',\n",
      "                              '--replica_count=2',\n",
      "                              '--experiment_name=jtv7-bert-tune',\n",
      "                              '--experiment_run=run-20230621-012602'],\n",
      "                     'image_uri': 'gcr.io/hybrid-vertex/imdb_bert'},\n",
      "  'machine_spec': {'accelerator_count': 1,\n",
      "                   'accelerator_type': 'NVIDIA_TESLA_T4',\n",
      "                   'machine_type': 'n1-standard-16'},\n",
      "  'replica_count': 1},\n",
      " {'container_spec': {'image_uri': 'us-docker.pkg.dev/vertex-ai-restricted/training/reductionserver:latest'},\n",
      "  'machine_spec': {'machine_type': 'n1-highcpu-16'},\n",
      "  'replica_count': 2}]\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "steps_per_epoch = 200\n",
    "eval_steps = 50\n",
    "\n",
    "PER_REPLICA_BATCH_SIZE = 32\n",
    "LEARNING_RATE= 0.001\n",
    "\n",
    "REDUCTION_SERVER_COUNT = 2\n",
    "REDUCTION_SERVER_MACHINE_TYPE = \"n1-highcpu-16\"\n",
    "\n",
    "training_data_path = f'{STAGING_BUCKET}/bert-finetuning/imdb/tfrecords/train'\n",
    "validation_data_path = f'{STAGING_BUCKET}/bert-finetuning/imdb/tfrecords/valid'\n",
    "testing_data_path = f'{STAGING_BUCKET}/bert-finetuning/imdb/tfrecords/test'\n",
    "job_id = 'job-{}'.format(datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
    "job_dir = f'{STAGING_BUCKET}/jobs/{job_id}'\n",
    "\n",
    "WORKER_ARGS = [\n",
    "    \"--epochs=\" + str(epochs)\n",
    "    , \"--steps_per_epoch=\" + str(steps_per_epoch)\n",
    "    , \"--eval_steps=\" + str(eval_steps)\n",
    "    , \"--per_replica_batch_size=\" + str(PER_REPLICA_BATCH_SIZE)\n",
    "    , \"--training_data_path=\" + training_data_path\n",
    "    , \"--validation_data_path=\" + validation_data_path\n",
    "    , \"--testing_data_path=\" + testing_data_path\n",
    "    , \"--job_dir=\" + job_dir\n",
    "    , f\"--strategy={DISTRIBUTION_STRATEGY}\"\n",
    "    , \"--auto_shard_policy=data\" # data auto\n",
    "    , f\"--job_id={job_id}\"\n",
    "    , f\"--TRAIN_GPU={TRAIN_GPU}\"\n",
    "    , f\"--TRAIN_NGPU={TRAIN_NGPU}\"\n",
    "    , f\"--reduction_cnt={REDUCTION_SERVER_COUNT}\"\n",
    "    , f\"--replica_count={REPLICA_COUNT}\"\n",
    "    , f\"--experiment_name={EXPERIMENT_NAME}\"\n",
    "    , f\"--experiment_run={RUN_NAME}\"\n",
    "    , f\"--learning_rate={LEARNING_RATE}\"\n",
    "    , f\"--tuning={HP_TUNING}\"\n",
    "]\n",
    "\n",
    "from utils import workerpool_specs\n",
    "\n",
    "WORKER_POOL_SPECS = workerpool_specs.prepare_worker_pool_specs(\n",
    "    image_uri=TRAIN_IMAGE,\n",
    "    args=WORKER_ARGS,\n",
    "    replica_count=REPLICA_COUNT,\n",
    "    machine_type=MACHINE_TYPE,\n",
    "    accelerator_count=TRAIN_NGPU,\n",
    "    accelerator_type=TRAIN_GPU,\n",
    "    reduction_server_count=REDUCTION_SERVER_COUNT,\n",
    "    reduction_server_machine_type=REDUCTION_SERVER_MACHINE_TYPE,\n",
    ")\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(WORKER_POOL_SPECS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8194b74-6038-4626-8d78-f9a6eacb0e13",
   "metadata": {},
   "source": [
    "### create tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fcd26393-3755-4cf7-b19a-6d5560c0d00f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "projects/934903580331/locations/us-central1/tensorboards/5728253270573449216\n"
     ]
    }
   ],
   "source": [
    "vertex_ai_tb = vertex_ai.Tensorboard.create()\n",
    "TENSORBOARD = vertex_ai_tb.gca_resource.name\n",
    "\n",
    "# TENSORBOARD=\"projects/934903580331/locations/us-central1/tensorboards/5728253270573449216\"\n",
    "\n",
    "print(TENSORBOARD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "feca486a-bfff-4c7f-8573-c113873f7bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.init(\n",
    "    experiment=EXPERIMENT_NAME\n",
    "    # , experiment_tensorboard=vertex_ai_tb\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c52be21-0012-4586-9490-d684e4c4b960",
   "metadata": {},
   "source": [
    "### submit train job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05431b44-322d-4e02-98d3-ac896bdd846b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCELERATOR = TRAIN_GPU.lower().replace(\"nvidia_\",\"\").replace(\"_\",\"-\")\n",
    "print(ACCELERATOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8e2b5534-58aa-4ade-91c5-911448ff3f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_mmr_job = vertex_ai.CustomJob(\n",
    "    display_name=f'imdb-bert-{DISTRIBUTION_STRATEGY}-{TRAIN_NGPU}-{ACCELERATOR}',\n",
    "    worker_pool_specs=WORKER_POOL_SPECS,\n",
    "    staging_bucket=f'{STAGING_BUCKET}/{EXPERIMENT_NAME}/{RUN_NAME}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "62af2d8c-5e4c-4adc-9cd8-0602883d5d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_mmr_job.run(\n",
    "    sync=False\n",
    "    , service_account=VERTEX_SA\n",
    "    , tensorboard=TENSORBOARD\n",
    "    , restart_job_on_worker_restart=False\n",
    "    , enable_web_access=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "90747067-f6d8-48ba-895e-a5f4c0c9f7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Name: imdb-bert-run-20230621-012602-multiworker-2-1-r\n",
      "Job Resource Name: projects/934903580331/locations/us-central1/customJobs/2179064783045656576\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Job Name: {custom_mmr_job.display_name}\")\n",
    "print(f\"Job Resource Name: {custom_mmr_job.resource_name}\\n\")\n",
    "# print(f\"Check training progress at {custom_m_job._dashboard_uri()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b32e8af-7d24-4cd1-8324-54f41b971d36",
   "metadata": {},
   "source": [
    "## 2 Replicas, 2 GPUs each + Reduction Server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c5eb66-b7d9-4716-b3a4-f5e89c764223",
   "metadata": {},
   "source": [
    "### set Experiment Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "bc5e0fc3-a165-4051-95eb-3634ad0ae4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT_NAME: jtv7-bert-tune\n",
      "RUN_NAME: run-20230621-012707\n"
     ]
    }
   ],
   "source": [
    "RUN_NAME = f'run-{time.strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "\n",
    "print(f\"EXPERIMENT_NAME: {EXPERIMENT_NAME}\")\n",
    "print(f\"RUN_NAME: {RUN_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1862456f-533c-46f9-bb99-8c8cbafeae90",
   "metadata": {},
   "source": [
    "### config compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "aca0c2a9-9de4-4e9d-adc9-e5fc9f161d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MACHINE_TYPE          : n1-standard-16\n",
      "TRAIN_GPU             : NVIDIA_TESLA_T4\n",
      "TRAIN_NGPU            : 2\n",
      "REPLICA_COUNT         : 2\n",
      "DISTRIBUTION_STRATEGY : multiworker\n"
     ]
    }
   ],
   "source": [
    "# MACHINE_TYPE = 'a2-highgpu-2g'\n",
    "# TRAIN_GPU, TRAIN_NGPU = ('NVIDIA_TESLA_A100', 2)\n",
    "\n",
    "# MACHINE_TYPE = 'n1-standard-16'\n",
    "# TRAIN_GPU, TRAIN_NGPU = ('NVIDIA_TESLA_T4', 2)\n",
    "\n",
    "# MACHINE_TYPE = 'a2-highgpu-2g'\n",
    "# TRAIN_GPU, TRAIN_NGPU = ('NVIDIA_TESLA_A100', 2)\n",
    "\n",
    "MACHINE_TYPE = 'a2-ultragpu-2g'\n",
    "TRAIN_GPU, TRAIN_NGPU = ('NVIDIA_A100_80GB', 2)\n",
    "\n",
    "REPLICA_COUNT = 2\n",
    "DISTRIBUTION_STRATEGY = \"multiworker\" # single, mirrored, multiworker, tpu\n",
    "\n",
    "HP_TUNING=\"False\"\n",
    "\n",
    "print(f\"MACHINE_TYPE          : {MACHINE_TYPE}\")\n",
    "print(f\"TRAIN_GPU             : {TRAIN_GPU}\")\n",
    "print(f\"TRAIN_NGPU            : {TRAIN_NGPU}\")\n",
    "print(f\"REPLICA_COUNT         : {REPLICA_COUNT}\")\n",
    "print(f\"DISTRIBUTION_STRATEGY : {DISTRIBUTION_STRATEGY}\")\n",
    "print(f\"HP_TUNING             : {HP_TUNING}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c63c62d-c178-4d60-8710-0ee8e13220f6",
   "metadata": {},
   "source": [
    "### worker args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "82326847-5b3e-48ec-87ce-d447f965ac3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'container_spec': {'args': ['--epochs=10',\n",
      "                              '--steps_per_epoch=200',\n",
      "                              '--eval_steps=50',\n",
      "                              '--per_replica_batch_size=32',\n",
      "                              '--training_data_path=gs://jtv7-hybrid-vertex-bucket/bert-finetuning/imdb/tfrecords/train',\n",
      "                              '--validation_data_path=gs://jtv7-hybrid-vertex-bucket/bert-finetuning/imdb/tfrecords/valid',\n",
      "                              '--testing_data_path=gs://jtv7-hybrid-vertex-bucket/bert-finetuning/imdb/tfrecords/test',\n",
      "                              '--job_dir=gs://jtv7-hybrid-vertex-bucket/jobs/job-20230621012708',\n",
      "                              '--strategy=multiworker',\n",
      "                              '--auto_shard_policy=data',\n",
      "                              '--job_id=job-20230621012708',\n",
      "                              '--TRAIN_GPU=NVIDIA_TESLA_T4',\n",
      "                              '--TRAIN_NGPU=2',\n",
      "                              '--reduction_cnt=2',\n",
      "                              '--replica_count=2',\n",
      "                              '--experiment_name=jtv7-bert-tune',\n",
      "                              '--experiment_run=run-20230621-012707'],\n",
      "                     'image_uri': 'gcr.io/hybrid-vertex/imdb_bert'},\n",
      "  'machine_spec': {'accelerator_count': 2,\n",
      "                   'accelerator_type': 'NVIDIA_TESLA_T4',\n",
      "                   'machine_type': 'n1-standard-16'},\n",
      "  'replica_count': 1},\n",
      " {'container_spec': {'args': ['--epochs=10',\n",
      "                              '--steps_per_epoch=200',\n",
      "                              '--eval_steps=50',\n",
      "                              '--per_replica_batch_size=32',\n",
      "                              '--training_data_path=gs://jtv7-hybrid-vertex-bucket/bert-finetuning/imdb/tfrecords/train',\n",
      "                              '--validation_data_path=gs://jtv7-hybrid-vertex-bucket/bert-finetuning/imdb/tfrecords/valid',\n",
      "                              '--testing_data_path=gs://jtv7-hybrid-vertex-bucket/bert-finetuning/imdb/tfrecords/test',\n",
      "                              '--job_dir=gs://jtv7-hybrid-vertex-bucket/jobs/job-20230621012708',\n",
      "                              '--strategy=multiworker',\n",
      "                              '--auto_shard_policy=data',\n",
      "                              '--job_id=job-20230621012708',\n",
      "                              '--TRAIN_GPU=NVIDIA_TESLA_T4',\n",
      "                              '--TRAIN_NGPU=2',\n",
      "                              '--reduction_cnt=2',\n",
      "                              '--replica_count=2',\n",
      "                              '--experiment_name=jtv7-bert-tune',\n",
      "                              '--experiment_run=run-20230621-012707'],\n",
      "                     'image_uri': 'gcr.io/hybrid-vertex/imdb_bert'},\n",
      "  'machine_spec': {'accelerator_count': 2,\n",
      "                   'accelerator_type': 'NVIDIA_TESLA_T4',\n",
      "                   'machine_type': 'n1-standard-16'},\n",
      "  'replica_count': 1},\n",
      " {'container_spec': {'image_uri': 'us-docker.pkg.dev/vertex-ai-restricted/training/reductionserver:latest'},\n",
      "  'machine_spec': {'machine_type': 'n1-highcpu-16'},\n",
      "  'replica_count': 2}]\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "steps_per_epoch = 200\n",
    "eval_steps = 50\n",
    "\n",
    "PER_REPLICA_BATCH_SIZE = 32\n",
    "LEARNING_RATE= 0.001\n",
    "\n",
    "REDUCTION_SERVER_COUNT = 2\n",
    "REDUCTION_SERVER_MACHINE_TYPE = \"n1-highcpu-16\"\n",
    "\n",
    "training_data_path = f'{STAGING_BUCKET}/bert-finetuning/imdb/tfrecords/train'\n",
    "validation_data_path = f'{STAGING_BUCKET}/bert-finetuning/imdb/tfrecords/valid'\n",
    "testing_data_path = f'{STAGING_BUCKET}/bert-finetuning/imdb/tfrecords/test'\n",
    "job_id = 'job-{}'.format(datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
    "job_dir = f'{STAGING_BUCKET}/jobs/{job_id}'\n",
    "\n",
    "WORKER_ARGS = [\n",
    "    \"--epochs=\" + str(epochs)\n",
    "    , \"--steps_per_epoch=\" + str(steps_per_epoch)\n",
    "    , \"--eval_steps=\" + str(eval_steps)\n",
    "    , \"--per_replica_batch_size=\" + str(PER_REPLICA_BATCH_SIZE)\n",
    "    , \"--training_data_path=\" + training_data_path\n",
    "    , \"--validation_data_path=\" + validation_data_path\n",
    "    , \"--testing_data_path=\" + testing_data_path\n",
    "    , \"--job_dir=\" + job_dir\n",
    "    , f\"--strategy={DISTRIBUTION_STRATEGY}\"\n",
    "    , \"--auto_shard_policy=data\" # data auto\n",
    "    , f\"--job_id={job_id}\"\n",
    "    , f\"--TRAIN_GPU={TRAIN_GPU}\"\n",
    "    , f\"--TRAIN_NGPU={TRAIN_NGPU}\"\n",
    "    , f\"--reduction_cnt={REDUCTION_SERVER_COUNT}\"\n",
    "    , f\"--replica_count={REPLICA_COUNT}\"\n",
    "    , f\"--experiment_name={EXPERIMENT_NAME}\"\n",
    "    , f\"--experiment_run={RUN_NAME}\"\n",
    "    , f\"--learning_rate={LEARNING_RATE}\"\n",
    "    , f\"--tuning={HP_TUNING}\"\n",
    "]\n",
    "\n",
    "from utils import workerpool_specs\n",
    "\n",
    "WORKER_POOL_SPECS = workerpool_specs.prepare_worker_pool_specs(\n",
    "    image_uri=TRAIN_IMAGE,\n",
    "    args=WORKER_ARGS,\n",
    "    replica_count=REPLICA_COUNT,\n",
    "    machine_type=MACHINE_TYPE,\n",
    "    accelerator_count=TRAIN_NGPU,\n",
    "    accelerator_type=TRAIN_GPU,\n",
    "    reduction_server_count=REDUCTION_SERVER_COUNT,\n",
    "    reduction_server_machine_type=REDUCTION_SERVER_MACHINE_TYPE,\n",
    ")\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(WORKER_POOL_SPECS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e15f65-391b-4d69-8cbc-5ac0ba549c12",
   "metadata": {},
   "source": [
    "### create tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9b034d8c-1fb4-4a68-8703-08c91e61727c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "projects/934903580331/locations/us-central1/tensorboards/6655994793811771392\n"
     ]
    }
   ],
   "source": [
    "vertex_ai_tb = vertex_ai.Tensorboard.create()\n",
    "TENSORBOARD = vertex_ai_tb.gca_resource.name\n",
    "\n",
    "# TENSORBOARD=\"projects/934903580331/locations/us-central1/tensorboards/3422410261359755264\"\n",
    "\n",
    "print(TENSORBOARD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "49dca337-cd7d-48da-be3e-a2b82c9b1fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.init(\n",
    "    experiment=EXPERIMENT_NAME\n",
    "    # , experiment_tensorboard=vertex_ai_tb\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5767a217-98bc-40f3-afae-9d2c999dba09",
   "metadata": {},
   "source": [
    "### submit train job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e92b08-dc21-444a-83f1-f4b9a0f9a12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCELERATOR = TRAIN_GPU.lower().replace(\"nvidia_\",\"\").replace(\"_\",\"-\")\n",
    "print(ACCELERATOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "13b0a5fe-7ef0-41c4-8075-f6c6b198c499",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_r4_job = vertex_ai.CustomJob(\n",
    "    display_name=f'imdb-bert-{DISTRIBUTION_STRATEGY}-{TRAIN_NGPU}-{ACCELERATOR}',\n",
    "    worker_pool_specs=WORKER_POOL_SPECS,\n",
    "    staging_bucket=f'{STAGING_BUCKET}/{EXPERIMENT_NAME}/{RUN_NAME}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "905ff012-2336-4bfe-9a04-0c2ed5487dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_r4_job.run(\n",
    "    sync=False\n",
    "    , service_account=VERTEX_SA\n",
    "    , tensorboard=TENSORBOARD\n",
    "    , restart_job_on_worker_restart=False\n",
    "    , enable_web_access=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "bfd1ec01-6d46-4380-a56d-2bb8ca0b3554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Name: imdb-bert-run-20230621-012707-multiworker-2-2-r\n",
      "Job Resource Name: projects/934903580331/locations/us-central1/customJobs/1688172423662272512\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Job Name: {custom_r4_job.display_name}\")\n",
    "print(f\"Job Resource Name: {custom_r4_job.resource_name}\\n\")\n",
    "# print(f\"Check training progress at {custom_m_job._dashboard_uri()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d259bfe-4341-4b47-a6e2-8c1976189b76",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Cloud TPU \n",
    "\n",
    "* To use [Tensor Processing Units (TPUs)](https://cloud.google.com/tpu/docs/tpus) for custom training on Vertex AI, you can configure a worker pool to use a [TPU VM](https://cloud.google.com/tpu/docs/system-architecture-tpu-vm#tpu-vm).\n",
    "\n",
    "When you use a TPU VM in Vertex AI, you must only use a single worker pool for custom training, and you must configure this worker pool to use only one replica."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f5f21b-31c1-4235-85e4-ba4d063a74e9",
   "metadata": {},
   "source": [
    "#### TPU VMs worker pool configurations:\n",
    "\n",
    "**To configure a TPU VM with [TPU V2](https://cloud.google.com/tpu/docs/system-architecture-tpu-vm#tpu_v2), specify the following fields in the WorkerPoolSpec:**\n",
    "* Set `machineSpec.machineType` to `cloud-tpu`.\n",
    "* Set `machineSpec.acceleratorType` to `TPU_V2`.\n",
    "* Set `machineSpec.acceleratorCount` to 8 for single TPU or `32 or multiple of 32` for TPU Pods.\n",
    "* Set `replicaCount` to 1.\n",
    "\n",
    "**To configure a TPU VM with [TPU V3](https://cloud.google.com/tpu/docs/system-architecture-tpu-vm#tpu_v3), specify the following fields in the WorkerPoolSpec:**\n",
    "* Set `machineSpec.machineType` to `cloud-tpu`.\n",
    "* Set `machineSpec.acceleratorType` to `TPU_V3`.\n",
    "* Set `machineSpec.acceleratorCount` to `8` for single TPU or `32+` for TPU Pods.\n",
    "* Set `replicaCount` to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "b7ac5c72-bdfc-408a-9d32-73af71005c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf scripts_tpu/trainer\n",
    "! mkdir -p scripts_tpu/trainer\n",
    "! touch scripts_tpu/trainer/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "312775dd-aa81-48ef-b560-2e189cb3185d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dockerfile  trainer\n"
     ]
    }
   ],
   "source": [
    "! cp -R scripts/trainer scripts_tpu/\n",
    "! ls scripts_tpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65de6133-3770-4834-8396-3f66b0df52b9",
   "metadata": {},
   "source": [
    "#### write Dockefile for TPU training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "0653d146-99a8-4d99-a0c2-d4e1b3d5a6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN_BASE_IMAGE = 'us-docker.pkg.dev/vertex-ai/training/tf-gpu.2-11:latest'\n",
    "# FROM us-docker.pkg.dev/vertex-ai/training/tf-tpu-pod-base-cp38:latest\n",
    "\n",
    "# TRAIN_IMAGE_TPU = gcr.io/hybrid-vertex/imdb_bert_tpu:latest\n",
    "DOCKER_TPU = 'Dockerfile'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "6d2148f3-36fd-42d9-aabd-08b4741bb2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dockerfile = f'''\n",
    "# FROM python:3.8\n",
    "\n",
    "# RUN pip install tf-models-official==2.12.0\n",
    "# RUN pip install tensorflow-text==2.12.0\n",
    "\n",
    "# WORKDIR /\n",
    "\n",
    "# # Copies the trainer code to the docker image.\n",
    "# COPY trainer /trainer\n",
    "\n",
    "# # Install TPU Tensorflow and dependencies.\n",
    "# # libtpu.so must be under the '/lib' directory.\n",
    "# RUN wget https://storage.googleapis.com/cloud-tpu-tpuvm-artifacts/libtpu/20221214/libtpu.so -O /lib/libtpu.so\n",
    "# RUN chmod 777 /lib/libtpu.so\n",
    "\n",
    "# RUN wget https://storage.googleapis.com/cloud-tpu-tpuvm-artifacts/tensorflow/20221214/tf_nightly-2.12.0-cp38-cp38-linux_x86_64.whl\n",
    "# RUN pip3 install tf_nightly-2.12.0-cp38-cp38-linux_x86_64.whl\n",
    "# RUN rm tf_nightly-2.12.0-cp38-cp38-linux_x86_64.whl\n",
    "\n",
    "# # Sets up the entry point to invoke the trainer.\n",
    "# ENTRYPOINT [\"python\", \"-m\", \"trainer.task\"]\n",
    "# '''\n",
    "\n",
    "# with open(f'scripts_tpu/{DOCKER_TPU}', 'w') as f:\n",
    "#     f.write(dockerfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "99198b72-5449-4b29-b798-0b236b8c568b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dockerfile = f'''\n",
    "FROM python:3.8\n",
    "\n",
    "WORKDIR /\n",
    "\n",
    "RUN wget https://storage.googleapis.com/cloud-tpu-tpuvm-artifacts/libtpu/1.5.0/libtpu.so -O /lib/libtpu.so\n",
    "RUN chmod 777 /lib/libtpu.so\n",
    "\n",
    "RUN wget https://storage.googleapis.com/cloud-tpu-tpuvm-artifacts/tensorflow/tf-2.11.0/tensorflow-2.11.0-cp38-cp38-linux_x86_64.whl\n",
    "RUN pip3 install tensorflow-2.11.0-cp38-cp38-linux_x86_64.whl\n",
    "RUN rm tensorflow-2.11.0-cp38-cp38-linux_x86_64.whl\n",
    "\n",
    "RUN pip install tf-models-official==2.11.0\n",
    "RUN pip install tensorflow-text==2.11.0\n",
    "RUN pip install cloudml-hypertune\n",
    "RUN pip install --upgrade tensorflow-hub\n",
    "RUN pip install --upgrade google-cloud-aiplatform\n",
    "\n",
    "# Copies the trainer code to the docker image.\n",
    "COPY trainer /trainer\n",
    "\n",
    "# Sets up the entry point to invoke the trainer.\n",
    "ENTRYPOINT [\"python\", \"-m\", \"trainer.task\"]\n",
    "'''\n",
    "\n",
    "with open(f'scripts_tpu/{DOCKER_TPU}', 'w') as f:\n",
    "     f.write(dockerfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d62de94-662f-406e-8db9-3a2c205ab78a",
   "metadata": {},
   "source": [
    "#### run docker commands in terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe952c61-4698-43cd-91ef-0f2779ee21d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! docker build -t $TRAIN_IMAGE_TPU scripts_tpu/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb91028-33fe-4f1c-889c-878024a60090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! docker push $TRAIN_IMAGE_TPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e672e5-1451-4126-b98b-ac50d54a8790",
   "metadata": {},
   "source": [
    "### set Experiment Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "be561f5e-add4-4bb5-82b7-731f09ae14b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT_NAME: jtv8-bert-tune\n",
      "RUN_NAME: run-20230621-122648\n"
     ]
    }
   ],
   "source": [
    "RUN_NAME = f'run-{time.strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "\n",
    "print(f\"EXPERIMENT_NAME: {EXPERIMENT_NAME}\")\n",
    "print(f\"RUN_NAME: {RUN_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec4ee60-4bbf-4b05-8b79-e46eaaab2656",
   "metadata": {},
   "source": [
    "### config compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "4e527ea3-58c9-4f63-932d-23d7f5b100dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tpu\n",
      "MACHINE_TYPE          : cloud-tpu\n",
      "TRAIN_TPU             : 6\n",
      "TRAIN_NTPU            : 8\n",
      "REPLICA_COUNT         : 1\n",
      "DISTRIBUTION_STRATEGY : tpu\n",
      "HP_TUNING             : False\n"
     ]
    }
   ],
   "source": [
    "# Use TPU Accelerators. Temporarily using numeric codes, until types are added to the SDK\n",
    "#   6 = TPU_V2\n",
    "#   7 = TPU_V3\n",
    "TRAIN_TPU, TRAIN_NTPU = (6, 8)\n",
    "MACHINE_TYPE = \"cloud-tpu\"\n",
    "\n",
    "# VM count\n",
    "REPLICA_COUNT = 1\n",
    "\n",
    "# distribution strategy\n",
    "if not TRAIN_NTPU or TRAIN_NTPU < 2:\n",
    "    DISTRIBUTION_STRATEGY = \"single\"\n",
    "else:\n",
    "    DISTRIBUTION_STRATEGY = \"tpu\"\n",
    "print(DISTRIBUTION_STRATEGY)\n",
    "\n",
    "# hptuning\n",
    "HP_TUNING=\"False\"\n",
    "\n",
    "print(f\"MACHINE_TYPE          : {MACHINE_TYPE}\")\n",
    "print(f\"TRAIN_TPU             : {TRAIN_TPU}\")\n",
    "print(f\"TRAIN_NTPU            : {TRAIN_NTPU}\")\n",
    "print(f\"REPLICA_COUNT         : {REPLICA_COUNT}\")\n",
    "print(f\"DISTRIBUTION_STRATEGY : {DISTRIBUTION_STRATEGY}\")\n",
    "print(f\"HP_TUNING             : {HP_TUNING}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a7cbb7-5ee0-4994-8d38-aadfa9d82fcf",
   "metadata": {},
   "source": [
    "### worker args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "b016c76f-f779-430b-b6f3-0871b63b7ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'container_spec': {'args': ['--epochs=10',\n",
      "                              '--steps_per_epoch=200',\n",
      "                              '--eval_steps=50',\n",
      "                              '--per_replica_batch_size=32',\n",
      "                              '--training_data_path=gs://jtv8-hybrid-vertex-bucket/bert-finetuning/imdb/tfrecords/train',\n",
      "                              '--validation_data_path=gs://jtv8-hybrid-vertex-bucket/bert-finetuning/imdb/tfrecords/valid',\n",
      "                              '--testing_data_path=gs://jtv8-hybrid-vertex-bucket/bert-finetuning/imdb/tfrecords/test',\n",
      "                              '--job_dir=gs://jtv8-hybrid-vertex-bucket/jobs/job-20230621124323',\n",
      "                              '--strategy=tpu',\n",
      "                              '--auto_shard_policy=data',\n",
      "                              '--job_id=job-20230621124323',\n",
      "                              '--TRAIN_GPU=6',\n",
      "                              '--TRAIN_NGPU=8',\n",
      "                              '--reduction_cnt=0',\n",
      "                              '--replica_count=1',\n",
      "                              '--experiment_name=jtv8-bert-tune',\n",
      "                              '--experiment_run=run-20230621-122648',\n",
      "                              '--learning_rate=0.001',\n",
      "                              '--tuning=False'],\n",
      "                     'image_uri': 'gcr.io/hybrid-vertex/imdb_bert_tpu:latest'},\n",
      "  'machine_spec': {'accelerator_count': 8,\n",
      "                   'accelerator_type': 6,\n",
      "                   'machine_type': 'cloud-tpu'},\n",
      "  'replica_count': 1}]\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "steps_per_epoch = 200\n",
    "eval_steps = 50\n",
    "\n",
    "PER_REPLICA_BATCH_SIZE = 32\n",
    "LEARNING_RATE= 0.001\n",
    "\n",
    "REDUCTION_SERVER_COUNT = 0\n",
    "REDUCTION_SERVER_MACHINE_TYPE = \"n1-highcpu-16\"\n",
    "\n",
    "training_data_path = f'{STAGING_BUCKET}/bert-finetuning/imdb/tfrecords/train'\n",
    "validation_data_path = f'{STAGING_BUCKET}/bert-finetuning/imdb/tfrecords/valid'\n",
    "testing_data_path = f'{STAGING_BUCKET}/bert-finetuning/imdb/tfrecords/test'\n",
    "job_id = 'job-{}'.format(datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
    "job_dir = f'{STAGING_BUCKET}/jobs/{job_id}'\n",
    "\n",
    "WORKER_ARGS = [\n",
    "    \"--epochs=\" + str(epochs)\n",
    "    , \"--steps_per_epoch=\" + str(steps_per_epoch)\n",
    "    , \"--eval_steps=\" + str(eval_steps)\n",
    "    , \"--per_replica_batch_size=\" + str(PER_REPLICA_BATCH_SIZE)\n",
    "    , \"--training_data_path=\" + training_data_path\n",
    "    , \"--validation_data_path=\" + validation_data_path\n",
    "    , \"--testing_data_path=\" + testing_data_path\n",
    "    , \"--job_dir=\" + job_dir\n",
    "    , f\"--strategy={DISTRIBUTION_STRATEGY}\"\n",
    "    , \"--auto_shard_policy=data\" # data auto\n",
    "    , f\"--job_id={job_id}\"\n",
    "    , f\"--TRAIN_GPU={TRAIN_TPU}\"\n",
    "    , f\"--TRAIN_NGPU={TRAIN_NTPU}\"\n",
    "    , f\"--reduction_cnt={REDUCTION_SERVER_COUNT}\"\n",
    "    , f\"--replica_count={REPLICA_COUNT}\"\n",
    "    , f\"--experiment_name={EXPERIMENT_NAME}\"\n",
    "    , f\"--experiment_run={RUN_NAME}\"\n",
    "    , f\"--learning_rate={LEARNING_RATE}\"\n",
    "    , f\"--tuning={HP_TUNING}\"\n",
    "]\n",
    "\n",
    "from utils import workerpool_specs\n",
    "\n",
    "WORKER_POOL_SPECS = workerpool_specs.prepare_worker_pool_specs(\n",
    "    image_uri=TRAIN_IMAGE_TPU,\n",
    "    args=WORKER_ARGS,\n",
    "    replica_count=REPLICA_COUNT,\n",
    "    machine_type=MACHINE_TYPE,\n",
    "    accelerator_count=TRAIN_NTPU,\n",
    "    accelerator_type=TRAIN_TPU,\n",
    "    reduction_server_count=REDUCTION_SERVER_COUNT,\n",
    "    reduction_server_machine_type=REDUCTION_SERVER_MACHINE_TYPE,\n",
    ")\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(WORKER_POOL_SPECS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd9e3a4-0108-45c7-bb92-511dc3afa3c8",
   "metadata": {},
   "source": [
    "### create tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "badaa5c2-51be-4458-96d9-055915e97629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "projects/934903580331/locations/us-central1/tensorboards/4318626587206483968\n"
     ]
    }
   ],
   "source": [
    "vertex_ai_tb = vertex_ai.Tensorboard.create()\n",
    "TENSORBOARD = vertex_ai_tb.gca_resource.name\n",
    "\n",
    "# TENSORBOARD=\"projects/934903580331/locations/us-central1/tensorboards/5728253270573449216\"\n",
    "\n",
    "print(TENSORBOARD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "2b252047-d7fd-4eba-80f6-338b52902e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.init(\n",
    "    experiment=EXPERIMENT_NAME\n",
    "    # , experiment_tensorboard=vertex_ai_tb\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb02b6bb-12e2-4ccc-b7f9-f3a66d912357",
   "metadata": {},
   "source": [
    "### submit train job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "9b1ff30d-52ea-41a0-8d21-25865c6018ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tpu-v2\n"
     ]
    }
   ],
   "source": [
    "ACCELERATOR = \"tpu-v2\"\n",
    "print(ACCELERATOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "f0770845-47a3-4a60-b2f5-d7767599bbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_tpu_job = vertex_ai.CustomJob(\n",
    "    display_name=f'imdb-bert-{DISTRIBUTION_STRATEGY}-{TRAIN_NTPU}-{ACCELERATOR}',\n",
    "    worker_pool_specs=WORKER_POOL_SPECS,\n",
    "    staging_bucket=f'{STAGING_BUCKET}/{EXPERIMENT_NAME}/{RUN_NAME}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "fde38375-34a8-406c-aab7-5e4cf20e7949",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_tpu_job.run(\n",
    "    sync=False\n",
    "    # , service_account=VERTEX_SA\n",
    "    # , tensorboard=TENSORBOARD\n",
    "    , restart_job_on_worker_restart=False\n",
    "    , enable_web_access=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "42980a73-a5d6-4944-94ed-5a12b5c8a52c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Name: imdb-bert-tpu-8-tpu-v2\n",
      "Job Resource Name: projects/934903580331/locations/us-central1/customJobs/6248242968235343872\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Job Name: {custom_tpu_job.display_name}\")\n",
    "print(f\"Job Resource Name: {custom_tpu_job.resource_name}\\n\")\n",
    "# print(f\"Check training progress at {custom_tpu_job._dashboard_uri()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f199de-a53a-4d13-bb82-31a340265d3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu110.m103",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m103"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
